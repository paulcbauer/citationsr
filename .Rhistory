"|confirm|support|in line with|based")
tf$signal_positive <- grepl(signal.words, tf$citation.case)
tf_group <- aggregate(tf$signal_positive, by=list(year=tf$year), FUN=mean, na.rm=TRUE)
p <- ggplot(tf_group, aes(x=as.numeric(year), y=x))
pq <- p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Proportion of citations with `positive' signal",
label=percent) + ggtitle(paste0("Citation cases: ", article))
f4 <- paste0(output, '/04-citations-with-positive-signal.pdf')
ggsave(pq, file=f4, height=4, width=6)
message("File generated: ", f4)
authors <- tokenize(toLower(c(tf$document, article)), removePunct=T, removeNumbers=T)
authors <- unique(unlist(authors))
# tokenizing
tokens <- tokenize(toLower(tf$citation.case), removePunct=T, removeNumbers=T)
# removing stopwords, author names, and other frequent words
tokens <- removeFeatures(tokens,
c(stopwords("english"), "other", "others", "see", "also", "u", authors))
# stemming?
#tokens <- lapply(tokens, wordstem)
# creating n-grams
ngrams <- lapply(tokens, ngrams, 1:3)
# putting it all back together...
ngrams <- unlist(lapply(ngrams, paste, collapse=" "))
# constructing the DFM
cit <- corpus(ngrams)
docnames(cit) <- paste0(1:nrow(tf), '_', tf$document)
# summary(cit)
citmat <- dfm(cit)
f5 <- paste0(output, '/05-citations-word-cloud.pdf')
pdf(f5, height=4, width=4)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Title of my first plot")
plot(citmat, rot.per=0, scale=c(3, .3), max.words=80)
dev.off()
message("File generated: ", f5)
library(citations)
library(citations)
library(citations)
library(citations)
setwd("~/Google Drive/2016_Quality_of_citations/")
library(citations)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/")
library(citations)
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
# Audretsch 1996
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Beck 1995
dir.create("output/beck_1995")
file <- "data/Beck 1995_citation_cases.csv"
article <- "Beck and Katz (1995)"
output <- "output/beck_1995"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Fearon 2003
dir.create("output/fearon_2003")
file <- "data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "output/fearon_2003"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Inglehart 2000
dir.create("output/inglehart_2000")
file <- "data/Inglehart 2000_citation_cases.csv"
article <- "Inglehart and Baker (2000)"
output <- "output/inglehart_2000"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Uzzi 1996
dir.create("output/uzzi_1996")
file <- "data/Uzzi 1996_citation_cases.csv"
article <- "Uzzi (1996)"
output <- "output/uzzi_1996"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Acemoglu 2001
dir.create("output/acemoglu_2001")
file <- "data/Acemoglu 2001_citation_cases.csv"
article <- "Acemoglu, Johnson & Robinson (2001)"
output <- "output/acemoglu_2001"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
library(citations)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/")
library(citations)
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
library(citations)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/")
library(citations)
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
library(citations)
x <- "This is a test if it works as well? with the question mark. or not."
searchterms <- paste("(\\.[^.]|\\?[^?])*searchtermhere[^.]*\\.", sep = "")
stringr::str_extract_all(x, paste(searchterms, collapse="|"))
stringr::str_extract_all(x, searchterms)
x
searchterms <- paste("(\\.|\\?)[^.]*searchtermhere[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
x <- ".This is a test if it works as well? with the question mark. or not."
searchterms <- paste("(\\.|\\?)[^.]*searchtermhere[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
x <- ".This is a test if it works as well? with searchtermhere the question mark. or not."
searchterms <- paste("(\\.|\\?)[^.]*", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
searchterms <- paste("(\\.[^.]*|\\?[^?]*)", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
searchterms <- paste("(\\?[^?]*)", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
searchterms <- paste("(\\.[^.]*|\\?[^.]*)", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
searchterms <- paste("(\\?[^.]*)", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
x <- "lksdjfl! sdflkj. sdfsdf? lskdjfldj. sldjdsl"
x <- stringr::str_replace_all(x, "\\?", "\\.\\?")
x
x <- stringr::str_replace_all(x, '\\\\"', "''")
x <- stringr::str_replace_all(x, "!", "\\.!")
x
library(citations)
library(citations)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/")
library(citations)
# Acemoglu 2001
dir.create("output/acemoglu_2001")
file <- "data/Acemoglu 2001_citation_cases.csv"
article <- "Acemoglu, Johnson & Robinson (2001)"
output <- "output/acemoglu_2001"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Audretsch 1996
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Beck 1995
dir.create("output/beck_1995")
file <- "data/Beck 1995_citation_cases.csv"
article <- "Beck and Katz (1995)"
output <- "output/beck_1995"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Fearon 2003
dir.create("output/fearon_2003")
file <- "data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "output/fearon_2003"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Inglehart 2000
dir.create("output/inglehart_2000")
file <- "data/Inglehart 2000_citation_cases.csv"
article <- "Inglehart and Baker (2000)"
output <- "output/inglehart_2000"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Uzzi 1996
dir.create("output/uzzi_1996")
file <- "data/Uzzi 1996_citation_cases.csv"
article <- "Uzzi (1996)"
output <- "output/uzzi_1996"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
setwd("C:/Users/pbauer/Desktop/ris files/scraped citations/Beck 1995")
file <- "Beck 1995_citations-1-500.csv"
gen_ris(file)
gen_ris <- function(file){
citations <- read.table(file, header=TRUE, stringsAsFactors = F)
for (i in 1:nrow(citations)){ #
fileConn <- file(paste(i, " - ", stringr::str_replace_all(citations$doi[i], "[;\\<\\>:/]", ""), ".ris", sep=""))
writeLines(c("TY  - JOUR",
#paste("AU  - ", citations$author[i], sep=""),
# paste("PY  - ", citations$year[i], sep=""),
#paste("JO  - ", citations$journal[i], sep=""),
paste("DO  - ", citations$doi[i], sep="")#,
#paste("TI  - ", citations$title[i], sep="")
), fileConn)
close(fileConn)
}
}
gen_ris(file)
file <- "Beck 1995_citations-501-1000.csv"
gen_ris(file)
setwd("C:/Users/pbauer/Desktop/ris files/scraped citations/Beck 1995")
file <- "Beck 1995_citations-1001-1500"
gen_ris(file)
ations/Beck 1995")
#
file <- "Beck 1995_citations-1001-1500.csv"
gen_ris(file)
file <- "Beck 1995_citations-1501-1634.csv"
gen_ris(file)
library(citations)
library(citations)
setwd("C:/Users/pbauer/Downloads")
publicationdata <- read.table("publicationdata.txt", sep=";",
header = T, stringsAsFactors = F)
for (i in 1:6){
# SPECIFY ARGUMENT FOR SINGLE FUNCTIONS
folder <- "docs"
study.title <- publicationdata[i,2]
authors <- publicationdata[i,3]
year <- publicationdata[i,4]
# EXTRACT CITATION CASES
extract_citation_cases(folder = folder,
authorname = authors,
studyyear = year)
}
study.title <- publicationdata[i,2]
authorname <- publicationdata[i,3]
studyyear <- publicationdata[i,4]
authorname
studyyear
authorname <- unlist(stringr::str_split(authorname, ","))
authorname <- gsub(" ", "", authorname)
length.authorname <- length(authorname)
# Single author
if(length.authorname==1){
searchterms <- paste(authorname[1], "(|\\'s|\\’s|\\’|\\')(|,)\\s{0,2}(|\\[|\\()" ,studyyear, "", sep="")
}
# Two authors
if(length.authorname==2){
searchterms <- paste(authorname[1], " (\\&|and) ", authorname[2], "(|\\'s|\\’s|\\’|\\')(|,)\\s{0,2}(|\\[|\\()" ,studyyear, "(\\s{0,2}(:|,)(?# Komma oder Doppelpunkt)\\s{0,2}(PAGE|)(?# Page kommt vor oder nicht)(\\s{0,2}|)(?# nochmal space oder nicht)\\d*(?# zahl mit länge 0 oder mehr)(\\]|\\)|)(?# schliesst mit versch klammer oder nicht)|)(?# seitenzahlen ja,nein, falls nein einfach klammer matchen)(\\]|\\)|)", sep="")
# paste(authorname[1], " and ", authorname[2], sep=""), # Without year!
}
# Three authors
if(length.authorname==3){
searchterms <- c(paste(authorname[1], ", ", authorname[2], ", & ", authorname[3], " ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", & ", authorname[3], ", ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", and ", authorname[3], ", ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", and ", authorname[3], " ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", and ", authorname[3], " ", "\\(" ,studyyear, "\\)", sep=""),
paste(authorname[1], " AND OTHERS, ",studyyear, sep=""),
paste(authorname[1], " AND OTHERS ",studyyear, sep=""),
paste(authorname[1], " AND OTHERS (" ,studyyear, ")", sep=""),
paste(authorname[1], " AND OTHERS, (" ,studyyear, ")", sep="")
)
}
# More than three authors
if(length.authorname>3){
searchterms <- c(
paste(authorname[1], " AND OTHERS", "(|,)\\s{0,2}(|\\[|\\()" ,studyyear, "(\\s{0,2}(:|,)(?# Komma oder Doppelpunkt)\\s{0,2}(PAGE|)(?# Page kommt vor oder nicht)(\\s{0,2}|)(?# nochmal space oder nicht)\\d*(?# zahl mit länge 0 oder mehr)(\\]|\\)|)(?# schliesst mit versch klammer oder nicht)|)(?# seitenzahlen ja,nein, falls nein einfach klammer matchen)(\\]|\\)|)", sep="")
)
}
#############################################
# List file names in folder (ONLY .TXT FILES)
file.names <- dir(paste("./", folder, sep = ""), pattern = "processed.txt")
# Generate file paths
file.paths <- paste(paste("./", folder, "/", sep = ""), file.names, sep="")
# Count number of files in folder
n.docs <- length(file.paths)
# Specify number of documents to assess by setting n.docs
if(!is.null(number)){n.docs <- number}
# Generate regex for search terms and dependency on scope
searchterms <- paste("\\.[^.]*", searchterms, "[^.]*\\.", sep = "")
# Change the regex if scope is broader, i.e. if sentence before and after should be
# extracted
if(!is.null(scope)){
searchterms <- paste(paste(rep("\\.[^.]*", scope), collapse=""),
searchterms,
paste(rep("[^.]*\\.", scope), collapse=""),
sep = "")
}
searchterms
all.docs.cit.cases <- NULL
for (i in 1:n.docs){
# Read in files
con <- file(file.paths[i], encoding = "UTF-8")
x <- readLines(con)
close(con)
# Extract sentences/lines that contain searchterms
cit.cases.doc.i <- stringr::str_extract_all(x, paste(searchterms, collapse="|"))
# Write them to list
all.docs.cit.cases[i] <- cit.cases.doc.i
# Counter
if(stringr::str_detect(as.character(i), "[0-9]*0")){cat(i, ".. ", sep="")}
}
# Get first estimate of number of citation cases
total.citation.cases <- sum(sapply(all.docs.cit.cases, length))
cat("\n For ", authorname, " we have identified ", total.citation.cases, " citation cases within ", n.docs, " documents.", sep="")
citation.data <- data.frame(document = 1:total.citation.cases, citation.case = 1:total.citation.cases)
citation.data[,1] <- rep(file.names, sapply(all.docs.cit.cases[1:length(file.names)], length))
# does it work with filenames here?
citation.data[,2] <- unlist(all.docs.cit.cases)
citation.data$document <- sub("\\s-$", "", stringr::str_extract(citation.data$document, "^.*-.*\\s-"))
if(!is.null(clean)){
citation.data$citation.case <- stringr::str_replace_all(citation.data$citation.case, "^\\.FOOTNOTE[0-9]{1,3}\\s", "^\\.\\s")
citation.data$citation.case <- stringr::str_replace_all(citation.data$citation.case, "^\\.\\s|^\\.\\?\\s|^\\.!\\s|^\\.\\s?[0-9]{1,3}\\s?", "")
}
write.table(citation.data, file =  paste("./", authorname, " ", studyyear, "_citation_cases.csv", sep = ""), sep=",")
paste("./", authorname, " ", studyyear, "_citation_cases.csv", sep = "")
authorname
paste(authorname, sep="")
paste(authorname, collapse="")
paste("./", paste(authorname, collapse=""), " ", studyyear, "_citation_cases.csv", sep = "")
paste("./", paste(authorname, collapse=""), "_", studyyear, "_citation_cases.csv", sep = "")
paste("./", paste(authorname, collapse=""), "_", studyyear, "_citation_cases.html", sep = "")
write.table(citation.data, file =  paste("./", paste(authorname, collapse=""), "_", studyyear, "_citation_cases.csv", sep = ""), sep=",")
print(xtable::xtable(citation.data),type='html',comment=FALSE, file=paste("./", paste(authorname, collapse=""), "_", studyyear, "_citation_cases.html", sep = ""))
library(citations)
library(citations)
publicationdata <- read.table("publicationdata.txt", sep=";",
header = T, stringsAsFactors = F)
for (i in 1:6){
# SPECIFY ARGUMENT FOR SINGLE FUNCTIONS
folder <- "docs"
study.title <- publicationdata[i,2]
authorname <- publicationdata[i,3]
studyyear <- publicationdata[i,4]
# EXTRACT CITATION CASES
extract_citation_cases(folder = folder,
authorname = authorname,
studyyear = studyyear)
}
setwd("C:/Users/pbauer/Downloads")
library(citations)
dir.create("output/acemoglu_2001")
dir.create("output/acemoglu_2001")
dir.create("/output/acemoglu_2001")
dir.create("./output/acemoglu_2001")
dir.create("output")
dir.create("output/acemoglu_2001")
file <- "AcemogluJohnsonRobinson_2001_citation_cases.csv"
article <- "Acemoglu, Johnson & Robinson (2001)"
output <- "output/acemoglu_2001"
analyze_citations(file, article, output)
setwd("C:/Users/pbauer/Downloads")
folder <- "docs"
files <- list.files(folder, pattern = "txt$", full.names = T)
files
files[1:length(files)]
files[1:10]
filename <- "docs/Acemoglu et al. 2014 - Institutions, Human Capital, and Development.txt"
# load packages
require(magrittr)
require(stringr)
require(dplyr)
# open file
con <- file(filename, encoding = encoding)
encoding = "UTF-8"
lines.import = 2000
rcrossref = TRUE
bibtex = FALSE
vars = NULL
# load packages
require(magrittr)
require(stringr)
require(dplyr)
# open file
con <- file(filename, encoding = encoding)
x <- readLines(con, warn = F, n = lines.import)
close(con) # close connection
x <- str_replace_all(x, "[\r\n\f]" , "")
x <- str_replace_all(x, "^[Dd]ownloaded from.+" , "")
x <- str_replace_all(x, "ﬁ", "fi")
x <- x[sapply(x, nchar) > 0]
doc_name <- basename(filename)
doc_size <- file.size(filename)
full_line <- NA
doi <- NA
meta_source <- NA
doi_guess <- NA
meta_dat <- data.frame(author = NA, title = NA, container.title = NA, volume = NA, issue = NA, created = NA, issued = NA, page = NA, publisher = NA, subject = NA, type = NA, URL = NA, DOI = NA, ISSN = NA, link = NA, reference.count = NA, score = NA, source = NA)
# identify DOI
doi <- stringr::str_extract(x, '\\b(10[.][0-9]{4,}(?:[.][0-9]+)*/(?:(?!["&\'])\\S)+)\\b') # taken from http://stackoverflow.com/questions/27910/finding-a-doi-in-a-document-or-page
doi <- doi[!is.na(doi)]
doi
check_nonascii <- tools::showNonASCII(doi)
doi_nonascii <- ifelse(length(check_nonascii) > 0, TRUE, FALSE)
if(length(doi)==0){
# significant_lines <- sort(table(x), decreasing = TRUE)[1:3] # header approach
# significant_lines_combined <- paste(names(frequent_lines), collapse = " ")
significant_lines_combined <- paste(x[1:20], collapse = " ") # first 20 lines approach
doi <- rcrossref::cr_works(query=significant_lines_combined, limit = 1)$data$DOI
doi_guess <- TRUE
}
if(length(doi)!=0 & doi_nonascii == FALSE){
# extract lines that contain identifiers of DOI
doi <- stringr::str_replace(doi, "^DOI |^DOI: |DOI:|^doi |^doi: |doi:|http://dx\\.doi\\.org/", "")
doi <- stringr::str_replace(doi, "_supp$|\\.supp$", "")
# take the first one
doi <- doi[1]
if (rcrossref == TRUE) {
# query crossref with DOI
if(bibtex == TRUE) {
crossref_bibtex <- rcrossref::cr_cn(dois = doi, format = "bibtex", style = "apa")
write(crossref_bibtex, paste0(filename, ".bib"))
}
crossref_df <- try(rcrossref::cr_works(dois = doi) %>% .$data %>% dplyr::select(matches("^author$|^title$|^container.title$|^volume$|^issue$|^created$|^issued$|^page$|^publisher$|^subject$|^type$|^URL$|^DOI$|^ISSN$|^reference.count$|^score$|^source$")))
if (class(crossref_df) != "try-error") {   # error handling if call caused an error
meta_dat <- crossref_df
}
meta_source <- "CrossRef"
}
}
if (!("author" %in% colnames(meta_dat))) { meta_dat$author <- NA }
if (!("title" %in% colnames(meta_dat))) { meta_dat$title <- NA }
if (!("container.title" %in% colnames(meta_dat))) { meta_dat$container.title <- NA }
if (!("volume" %in% colnames(meta_dat))) { meta_dat$volume <- NA }
if (!("issue" %in% colnames(meta_dat))) { meta_dat$issue <- NA }
if (!("created" %in% colnames(meta_dat))) { meta_dat$created <- NA }
if (!("issued" %in% colnames(meta_dat))) { meta_dat$issued <- NA }
if (!("page" %in% colnames(meta_dat))) { meta_dat$page <- NA }
if (!("publisher" %in% colnames(meta_dat))) { meta_dat$publisher <- NA }
if (!("subject" %in% colnames(meta_dat))) { meta_dat$subject <- NA }
if (!("type" %in% colnames(meta_dat))) { meta_dat$type <- NA }
if (!("URL" %in% colnames(meta_dat))) { meta_dat$URL <- NA }
if (!("DOI" %in% colnames(meta_dat))) { meta_dat$DOI <- NA }
if (!("ISSN" %in% colnames(meta_dat))) { meta_dat$ISSN <- NA }
if (!("reference.count" %in% colnames(meta_dat))) { meta_dat$reference.count <- NA }
if (!("score" %in% colnames(meta_dat))) { meta_dat$score <- NA }
if (!("source" %in% colnames(meta_dat))) { meta_dat$source <- NA }
meta_dat$meta_source <- meta_source
meta_dat$doc_name <- doc_name
meta_dat$doc_size <- doc_size
meta_dat$doi_guess <- doi_guess
meta_dat$journal <- meta_dat$container.title
meta_dat$date1 <- meta_dat$created
meta_dat$date2 <- meta_dat$issued
meta_dat$doc_year <- meta_dat$doc_name %>% str_extract("[[:digit:]]{4}")
meta_dat$doc_author <- meta_dat$doc_name %>% str_extract("^[[:alpha:]- .]+([[:digit:]]{4}){1}") %>% str_replace("[[:digit:]]{4}", "")
meta_dat
require(magrittr)
require(stringr)
require(dplyr)
# open file
con <- file(filename, encoding = encoding)
# import text lines; lines.import specifies number of lines
x <- readLines(con, warn = F, n = lines.import)
close(con) # close connection
# remove form feed, line feed, carriage returns and empty lines from doc
x <- str_replace_all(x, "[\r\n\f]" , "")
x <- str_replace_all(x, "^[Dd]ownloaded from.+" , "")
x <- str_replace_all(x, "ﬁ", "fi")
x <- x[sapply(x, nchar) > 0]
# generate empty containers
doc_name <- basename(filename)
doc_size <- file.size(filename)
full_line <- NA
doi <- NA
meta_source <- NA
doi_guess <- NA
meta_dat <- data.frame(author = NA, title = NA, container.title = NA, volume = NA, issue = NA, created = NA, issued = NA, page = NA, publisher = NA, subject = NA, type = NA, URL = NA, DOI = NA, ISSN = NA, link = NA, reference.count = NA, score = NA, source = NA)
# identify DOI
doi <- stringr::str_extract(x, '\\b(10[.][0-9]{4,}(?:[.][0-9]+)*/(?:(?!["&\'])\\S)+)\\b') # taken from http://stackoverflow.com/questions/27910/finding-a-doi-in-a-document-or-page
doi <- doi[!is.na(doi)]
# check if DOI contains non-ASCII characters
check_nonascii <- tools::showNonASCII(doi)
doi_nonascii <- ifelse(length(check_nonascii) > 0, TRUE, FALSE)
# if no DOI available, try fetch it from CrossRef via information from first 20 lines
# detect frequent lines (headers?)
if(length(doi)==0){
# significant_lines <- sort(table(x), decreasing = TRUE)[1:3] # header approach
# significant_lines_combined <- paste(names(frequent_lines), collapse = " ")
significant_lines_combined <- paste(x[1:20], collapse = " ") # first 20 lines approach
doi <- rcrossref::cr_works(query=significant_lines_combined, limit = 1)$data$DOI
doi_guess <- TRUE
}
# if DOI available, query info from CrossRef
if(length(doi)!=0 & doi_nonascii == FALSE){
# extract lines that contain identifiers of DOI
doi <- stringr::str_replace(doi, "^DOI |^DOI: |DOI:|^doi |^doi: |doi:|http://dx\\.doi\\.org/", "")
doi <- stringr::str_replace(doi, "_supp$|\\.supp$", "")
# take the first one
doi <- doi[1]
if (rcrossref == TRUE) {
# query crossref with DOI
if(bibtex == TRUE) {
crossref_bibtex <- rcrossref::cr_cn(dois = doi, format = "bibtex", style = "apa")
write(crossref_bibtex, paste0(filename, ".bib"))
}
crossref_df <- try(rcrossref::cr_works(dois = doi) %>% .$data %>% dplyr::select(matches("^author$|^title$|^container.title$|^volume$|^issue$|^created$|^issued$|^page$|^publisher$|^subject$|^type$|^URL$|^DOI$|^ISSN$|^reference.count$|^score$|^source$")))
if (class(crossref_df) != "try-error") {   # error handling if call caused an error
meta_dat <- crossref_df
}
meta_source <- "CrossRef"
}
}
meta_source
crossref_df
rcrossref::cr_works(dois = doi)
doi
cat(cr_cn(dois = "10.1146/annurev-economics-080213-041119", format = "bibtex"))
cat(rcrossref::cr_cn(dois = "10.1146/annurev-economics-080213-041119", format = "bibtex"))
rcrossref::cr_works(dois = doi)
rcrossref::cr_cn(dois = doi)
library(rcrossref)
?cr_works
?cr_cn
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data")
setwd("C:/Users/pbauer/Google Drive/Research/2016_06_Quality_of_citations/data")
folder <- "docs"
