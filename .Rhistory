article <- "Uzzi (1996)"
output <- "output/uzzi_1996"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
setwd("C:/Users/pbauer/Desktop/ris files/scraped citations/Beck 1995")
file <- "Beck 1995_citations-1-500.csv"
gen_ris(file)
gen_ris <- function(file){
citations <- read.table(file, header=TRUE, stringsAsFactors = F)
for (i in 1:nrow(citations)){ #
fileConn <- file(paste(i, " - ", stringr::str_replace_all(citations$doi[i], "[;\\<\\>:/]", ""), ".ris", sep=""))
writeLines(c("TY  - JOUR",
#paste("AU  - ", citations$author[i], sep=""),
# paste("PY  - ", citations$year[i], sep=""),
#paste("JO  - ", citations$journal[i], sep=""),
paste("DO  - ", citations$doi[i], sep="")#,
#paste("TI  - ", citations$title[i], sep="")
), fileConn)
close(fileConn)
}
}
gen_ris(file)
file <- "Beck 1995_citations-501-1000.csv"
gen_ris(file)
setwd("C:/Users/pbauer/Desktop/ris files/scraped citations/Beck 1995")
file <- "Beck 1995_citations-1001-1500"
gen_ris(file)
ations/Beck 1995")
#
file <- "Beck 1995_citations-1001-1500.csv"
gen_ris(file)
file <- "Beck 1995_citations-1501-1634.csv"
gen_ris(file)
library(citations)
library(citations)
setwd("C:/Users/pbauer/Downloads")
publicationdata <- read.table("publicationdata.txt", sep=";",
header = T, stringsAsFactors = F)
for (i in 1:6){
# SPECIFY ARGUMENT FOR SINGLE FUNCTIONS
folder <- "docs"
study.title <- publicationdata[i,2]
authors <- publicationdata[i,3]
year <- publicationdata[i,4]
# EXTRACT CITATION CASES
extract_citation_cases(folder = folder,
authorname = authors,
studyyear = year)
}
study.title <- publicationdata[i,2]
authorname <- publicationdata[i,3]
studyyear <- publicationdata[i,4]
authorname
studyyear
authorname <- unlist(stringr::str_split(authorname, ","))
authorname <- gsub(" ", "", authorname)
length.authorname <- length(authorname)
# Single author
if(length.authorname==1){
searchterms <- paste(authorname[1], "(|\\'s|\\’s|\\’|\\')(|,)\\s{0,2}(|\\[|\\()" ,studyyear, "", sep="")
}
# Two authors
if(length.authorname==2){
searchterms <- paste(authorname[1], " (\\&|and) ", authorname[2], "(|\\'s|\\’s|\\’|\\')(|,)\\s{0,2}(|\\[|\\()" ,studyyear, "(\\s{0,2}(:|,)(?# Komma oder Doppelpunkt)\\s{0,2}(PAGE|)(?# Page kommt vor oder nicht)(\\s{0,2}|)(?# nochmal space oder nicht)\\d*(?# zahl mit länge 0 oder mehr)(\\]|\\)|)(?# schliesst mit versch klammer oder nicht)|)(?# seitenzahlen ja,nein, falls nein einfach klammer matchen)(\\]|\\)|)", sep="")
# paste(authorname[1], " and ", authorname[2], sep=""), # Without year!
}
# Three authors
if(length.authorname==3){
searchterms <- c(paste(authorname[1], ", ", authorname[2], ", & ", authorname[3], " ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", & ", authorname[3], ", ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", and ", authorname[3], ", ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", and ", authorname[3], " ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", and ", authorname[3], " ", "\\(" ,studyyear, "\\)", sep=""),
paste(authorname[1], " AND OTHERS, ",studyyear, sep=""),
paste(authorname[1], " AND OTHERS ",studyyear, sep=""),
paste(authorname[1], " AND OTHERS (" ,studyyear, ")", sep=""),
paste(authorname[1], " AND OTHERS, (" ,studyyear, ")", sep="")
)
}
# More than three authors
if(length.authorname>3){
searchterms <- c(
paste(authorname[1], " AND OTHERS", "(|,)\\s{0,2}(|\\[|\\()" ,studyyear, "(\\s{0,2}(:|,)(?# Komma oder Doppelpunkt)\\s{0,2}(PAGE|)(?# Page kommt vor oder nicht)(\\s{0,2}|)(?# nochmal space oder nicht)\\d*(?# zahl mit länge 0 oder mehr)(\\]|\\)|)(?# schliesst mit versch klammer oder nicht)|)(?# seitenzahlen ja,nein, falls nein einfach klammer matchen)(\\]|\\)|)", sep="")
)
}
#############################################
# List file names in folder (ONLY .TXT FILES)
file.names <- dir(paste("./", folder, sep = ""), pattern = "processed.txt")
# Generate file paths
file.paths <- paste(paste("./", folder, "/", sep = ""), file.names, sep="")
# Count number of files in folder
n.docs <- length(file.paths)
# Specify number of documents to assess by setting n.docs
if(!is.null(number)){n.docs <- number}
# Generate regex for search terms and dependency on scope
searchterms <- paste("\\.[^.]*", searchterms, "[^.]*\\.", sep = "")
# Change the regex if scope is broader, i.e. if sentence before and after should be
# extracted
if(!is.null(scope)){
searchterms <- paste(paste(rep("\\.[^.]*", scope), collapse=""),
searchterms,
paste(rep("[^.]*\\.", scope), collapse=""),
sep = "")
}
searchterms
all.docs.cit.cases <- NULL
for (i in 1:n.docs){
# Read in files
con <- file(file.paths[i], encoding = "UTF-8")
x <- readLines(con)
close(con)
# Extract sentences/lines that contain searchterms
cit.cases.doc.i <- stringr::str_extract_all(x, paste(searchterms, collapse="|"))
# Write them to list
all.docs.cit.cases[i] <- cit.cases.doc.i
# Counter
if(stringr::str_detect(as.character(i), "[0-9]*0")){cat(i, ".. ", sep="")}
}
# Get first estimate of number of citation cases
total.citation.cases <- sum(sapply(all.docs.cit.cases, length))
cat("\n For ", authorname, " we have identified ", total.citation.cases, " citation cases within ", n.docs, " documents.", sep="")
citation.data <- data.frame(document = 1:total.citation.cases, citation.case = 1:total.citation.cases)
citation.data[,1] <- rep(file.names, sapply(all.docs.cit.cases[1:length(file.names)], length))
# does it work with filenames here?
citation.data[,2] <- unlist(all.docs.cit.cases)
citation.data$document <- sub("\\s-$", "", stringr::str_extract(citation.data$document, "^.*-.*\\s-"))
if(!is.null(clean)){
citation.data$citation.case <- stringr::str_replace_all(citation.data$citation.case, "^\\.FOOTNOTE[0-9]{1,3}\\s", "^\\.\\s")
citation.data$citation.case <- stringr::str_replace_all(citation.data$citation.case, "^\\.\\s|^\\.\\?\\s|^\\.!\\s|^\\.\\s?[0-9]{1,3}\\s?", "")
}
write.table(citation.data, file =  paste("./", authorname, " ", studyyear, "_citation_cases.csv", sep = ""), sep=",")
paste("./", authorname, " ", studyyear, "_citation_cases.csv", sep = "")
authorname
paste(authorname, sep="")
paste(authorname, collapse="")
paste("./", paste(authorname, collapse=""), " ", studyyear, "_citation_cases.csv", sep = "")
paste("./", paste(authorname, collapse=""), "_", studyyear, "_citation_cases.csv", sep = "")
paste("./", paste(authorname, collapse=""), "_", studyyear, "_citation_cases.html", sep = "")
write.table(citation.data, file =  paste("./", paste(authorname, collapse=""), "_", studyyear, "_citation_cases.csv", sep = ""), sep=",")
print(xtable::xtable(citation.data),type='html',comment=FALSE, file=paste("./", paste(authorname, collapse=""), "_", studyyear, "_citation_cases.html", sep = ""))
library(citations)
library(citations)
publicationdata <- read.table("publicationdata.txt", sep=";",
header = T, stringsAsFactors = F)
for (i in 1:6){
# SPECIFY ARGUMENT FOR SINGLE FUNCTIONS
folder <- "docs"
study.title <- publicationdata[i,2]
authorname <- publicationdata[i,3]
studyyear <- publicationdata[i,4]
# EXTRACT CITATION CASES
extract_citation_cases(folder = folder,
authorname = authorname,
studyyear = studyyear)
}
setwd("C:/Users/pbauer/Downloads")
library(citations)
dir.create("output/acemoglu_2001")
dir.create("output/acemoglu_2001")
dir.create("/output/acemoglu_2001")
dir.create("./output/acemoglu_2001")
dir.create("output")
dir.create("output/acemoglu_2001")
file <- "AcemogluJohnsonRobinson_2001_citation_cases.csv"
article <- "Acemoglu, Johnson & Robinson (2001)"
output <- "output/acemoglu_2001"
analyze_citations(file, article, output)
setwd("C:/Users/pbauer/Downloads")
folder <- "docs"
files <- list.files(folder, pattern = "txt$", full.names = T)
files
files[1:length(files)]
files[1:10]
filename <- "docs/Acemoglu et al. 2014 - Institutions, Human Capital, and Development.txt"
# load packages
require(magrittr)
require(stringr)
require(dplyr)
# open file
con <- file(filename, encoding = encoding)
encoding = "UTF-8"
lines.import = 2000
rcrossref = TRUE
bibtex = FALSE
vars = NULL
# load packages
require(magrittr)
require(stringr)
require(dplyr)
# open file
con <- file(filename, encoding = encoding)
x <- readLines(con, warn = F, n = lines.import)
close(con) # close connection
x <- str_replace_all(x, "[\r\n\f]" , "")
x <- str_replace_all(x, "^[Dd]ownloaded from.+" , "")
x <- str_replace_all(x, "ﬁ", "fi")
x <- x[sapply(x, nchar) > 0]
doc_name <- basename(filename)
doc_size <- file.size(filename)
full_line <- NA
doi <- NA
meta_source <- NA
doi_guess <- NA
meta_dat <- data.frame(author = NA, title = NA, container.title = NA, volume = NA, issue = NA, created = NA, issued = NA, page = NA, publisher = NA, subject = NA, type = NA, URL = NA, DOI = NA, ISSN = NA, link = NA, reference.count = NA, score = NA, source = NA)
# identify DOI
doi <- stringr::str_extract(x, '\\b(10[.][0-9]{4,}(?:[.][0-9]+)*/(?:(?!["&\'])\\S)+)\\b') # taken from http://stackoverflow.com/questions/27910/finding-a-doi-in-a-document-or-page
doi <- doi[!is.na(doi)]
doi
check_nonascii <- tools::showNonASCII(doi)
doi_nonascii <- ifelse(length(check_nonascii) > 0, TRUE, FALSE)
if(length(doi)==0){
# significant_lines <- sort(table(x), decreasing = TRUE)[1:3] # header approach
# significant_lines_combined <- paste(names(frequent_lines), collapse = " ")
significant_lines_combined <- paste(x[1:20], collapse = " ") # first 20 lines approach
doi <- rcrossref::cr_works(query=significant_lines_combined, limit = 1)$data$DOI
doi_guess <- TRUE
}
if(length(doi)!=0 & doi_nonascii == FALSE){
# extract lines that contain identifiers of DOI
doi <- stringr::str_replace(doi, "^DOI |^DOI: |DOI:|^doi |^doi: |doi:|http://dx\\.doi\\.org/", "")
doi <- stringr::str_replace(doi, "_supp$|\\.supp$", "")
# take the first one
doi <- doi[1]
if (rcrossref == TRUE) {
# query crossref with DOI
if(bibtex == TRUE) {
crossref_bibtex <- rcrossref::cr_cn(dois = doi, format = "bibtex", style = "apa")
write(crossref_bibtex, paste0(filename, ".bib"))
}
crossref_df <- try(rcrossref::cr_works(dois = doi) %>% .$data %>% dplyr::select(matches("^author$|^title$|^container.title$|^volume$|^issue$|^created$|^issued$|^page$|^publisher$|^subject$|^type$|^URL$|^DOI$|^ISSN$|^reference.count$|^score$|^source$")))
if (class(crossref_df) != "try-error") {   # error handling if call caused an error
meta_dat <- crossref_df
}
meta_source <- "CrossRef"
}
}
if (!("author" %in% colnames(meta_dat))) { meta_dat$author <- NA }
if (!("title" %in% colnames(meta_dat))) { meta_dat$title <- NA }
if (!("container.title" %in% colnames(meta_dat))) { meta_dat$container.title <- NA }
if (!("volume" %in% colnames(meta_dat))) { meta_dat$volume <- NA }
if (!("issue" %in% colnames(meta_dat))) { meta_dat$issue <- NA }
if (!("created" %in% colnames(meta_dat))) { meta_dat$created <- NA }
if (!("issued" %in% colnames(meta_dat))) { meta_dat$issued <- NA }
if (!("page" %in% colnames(meta_dat))) { meta_dat$page <- NA }
if (!("publisher" %in% colnames(meta_dat))) { meta_dat$publisher <- NA }
if (!("subject" %in% colnames(meta_dat))) { meta_dat$subject <- NA }
if (!("type" %in% colnames(meta_dat))) { meta_dat$type <- NA }
if (!("URL" %in% colnames(meta_dat))) { meta_dat$URL <- NA }
if (!("DOI" %in% colnames(meta_dat))) { meta_dat$DOI <- NA }
if (!("ISSN" %in% colnames(meta_dat))) { meta_dat$ISSN <- NA }
if (!("reference.count" %in% colnames(meta_dat))) { meta_dat$reference.count <- NA }
if (!("score" %in% colnames(meta_dat))) { meta_dat$score <- NA }
if (!("source" %in% colnames(meta_dat))) { meta_dat$source <- NA }
meta_dat$meta_source <- meta_source
meta_dat$doc_name <- doc_name
meta_dat$doc_size <- doc_size
meta_dat$doi_guess <- doi_guess
meta_dat$journal <- meta_dat$container.title
meta_dat$date1 <- meta_dat$created
meta_dat$date2 <- meta_dat$issued
meta_dat$doc_year <- meta_dat$doc_name %>% str_extract("[[:digit:]]{4}")
meta_dat$doc_author <- meta_dat$doc_name %>% str_extract("^[[:alpha:]- .]+([[:digit:]]{4}){1}") %>% str_replace("[[:digit:]]{4}", "")
meta_dat
require(magrittr)
require(stringr)
require(dplyr)
# open file
con <- file(filename, encoding = encoding)
# import text lines; lines.import specifies number of lines
x <- readLines(con, warn = F, n = lines.import)
close(con) # close connection
# remove form feed, line feed, carriage returns and empty lines from doc
x <- str_replace_all(x, "[\r\n\f]" , "")
x <- str_replace_all(x, "^[Dd]ownloaded from.+" , "")
x <- str_replace_all(x, "ﬁ", "fi")
x <- x[sapply(x, nchar) > 0]
# generate empty containers
doc_name <- basename(filename)
doc_size <- file.size(filename)
full_line <- NA
doi <- NA
meta_source <- NA
doi_guess <- NA
meta_dat <- data.frame(author = NA, title = NA, container.title = NA, volume = NA, issue = NA, created = NA, issued = NA, page = NA, publisher = NA, subject = NA, type = NA, URL = NA, DOI = NA, ISSN = NA, link = NA, reference.count = NA, score = NA, source = NA)
# identify DOI
doi <- stringr::str_extract(x, '\\b(10[.][0-9]{4,}(?:[.][0-9]+)*/(?:(?!["&\'])\\S)+)\\b') # taken from http://stackoverflow.com/questions/27910/finding-a-doi-in-a-document-or-page
doi <- doi[!is.na(doi)]
# check if DOI contains non-ASCII characters
check_nonascii <- tools::showNonASCII(doi)
doi_nonascii <- ifelse(length(check_nonascii) > 0, TRUE, FALSE)
# if no DOI available, try fetch it from CrossRef via information from first 20 lines
# detect frequent lines (headers?)
if(length(doi)==0){
# significant_lines <- sort(table(x), decreasing = TRUE)[1:3] # header approach
# significant_lines_combined <- paste(names(frequent_lines), collapse = " ")
significant_lines_combined <- paste(x[1:20], collapse = " ") # first 20 lines approach
doi <- rcrossref::cr_works(query=significant_lines_combined, limit = 1)$data$DOI
doi_guess <- TRUE
}
# if DOI available, query info from CrossRef
if(length(doi)!=0 & doi_nonascii == FALSE){
# extract lines that contain identifiers of DOI
doi <- stringr::str_replace(doi, "^DOI |^DOI: |DOI:|^doi |^doi: |doi:|http://dx\\.doi\\.org/", "")
doi <- stringr::str_replace(doi, "_supp$|\\.supp$", "")
# take the first one
doi <- doi[1]
if (rcrossref == TRUE) {
# query crossref with DOI
if(bibtex == TRUE) {
crossref_bibtex <- rcrossref::cr_cn(dois = doi, format = "bibtex", style = "apa")
write(crossref_bibtex, paste0(filename, ".bib"))
}
crossref_df <- try(rcrossref::cr_works(dois = doi) %>% .$data %>% dplyr::select(matches("^author$|^title$|^container.title$|^volume$|^issue$|^created$|^issued$|^page$|^publisher$|^subject$|^type$|^URL$|^DOI$|^ISSN$|^reference.count$|^score$|^source$")))
if (class(crossref_df) != "try-error") {   # error handling if call caused an error
meta_dat <- crossref_df
}
meta_source <- "CrossRef"
}
}
meta_source
crossref_df
rcrossref::cr_works(dois = doi)
doi
cat(cr_cn(dois = "10.1146/annurev-economics-080213-041119", format = "bibtex"))
cat(rcrossref::cr_cn(dois = "10.1146/annurev-economics-080213-041119", format = "bibtex"))
rcrossref::cr_works(dois = doi)
rcrossref::cr_cn(dois = doi)
library(rcrossref)
?cr_works
?cr_cn
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data")
setwd("C:/Users/pbauer/Google Drive/Research/2016_06_Quality_of_citations/data")
folder <- "docs"
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data")
setwd("C:/Users/pbauer/Google Drive/Research/2016_06_Quality_of_citations/data")
folder <- "docs"
extract_text(folder = folder, number = 20)
library(citations)
extract_text(folder = folder, number = 20)
get_metadata_doc_nv(folder, number = 20)
delete_refs_n_heads(folder = folder, number = 20)
clean_text(folder = folder, number = 20)
setwd("C:\\Users\\Paul\\GDrive\\Research\\2016_06_Quality_of_citations\\data")
# setwd("C:/Users/pbauer/Google Drive/Research/2016_06_Quality_of_citations/data")
### Set folder ###
folder <- "docs"
### Text extraction ###
extract_text(folder = folder, number = 100)
get_metadata_doc_nv(folder, number = 20)
folder <- "docs"
### Text extraction ###
extract_text(folder = folder, number = 20)
library(citations)
library(citations)
library(citations)
library(citations)
library(mvtnorm)
K=3:8
runs=2
max.em.its=10
net.max.em.its=2
seed=777
require(ggplot2)
require(scales)
require(quanteda)
require(stm)
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
?scan
getwd()
file <- "~/Google Drive/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "fearon_2003"
require(ggplot2)
require(scales)
require(quanteda)
require(stm)
# precleaning file
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
text <- scan(file, what="character", sep="\n")
text <- scan(file, what="character", sep="\n")
file
text <- scan(file, what="character", sep="\n")
setwd("C:/Users/Paul/GDrive/Research/2016_06_Quality_of_citations")
file <- "data/AcemogluJohnsonRobinson_2001_citation_cases.csv"
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1")
tempfile()
text
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1")
tempfile()
library(citations)
library(citations)
setwd("C:/Users/Paul/GDrive/Research/2016_06_Quality_of_citations")
library(citations)
dir.create("output/audretsch_1996")
file <- "data/AudretschFeldman_1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
getwd()
getwd()
library(truncnorm)
library(GenOrd)
library(mvtnorm)
library(tmvtnorm)
library(scatterplot3d)
library(shiny)
library(moments)
library(rgl)
library(rglwidget)
library(leaflet)
library(haven)
library(plyr)
library(dplyr)
library(rgdal)
library(stargazer)
# Start the clock!
n <- 1000 # set number of draws
set.seed(777) # set seed
standarddevs <- seq(0.001,20,0.1)
correlations <- seq(0,0.9,0.1)
objectlength <- length(standarddevs)*length(correlations)
objectlength
# Generate vectors for both sds and correlations
sdsvector <- rep(standarddevs, each = length(correlations))
corrsvector <- rep(correlations, length(standarddevs))
# Create storing objects
sims <- sapply(paste(1:objectlength),function(x) NULL)
dist <- sapply(paste(1:objectlength),function(x) NULL)
R <- sapply(paste(1:objectlength),function(x) NULL)
# library(compiler)
# enableJIT(3)
Rprof("file.out")
ptm <- proc.time()
for(i in 1:objectlength){ # loop across SD
print(i)
# Draw and sort numbers for cummulative probabilites
dist[[i]] <- sort(rtruncnorm(5, a=0, b=1, mean = 0.5, sd = sdsvector[i]))
# Generate correlation matrix
R[[i]] <- matrix(c(1, corrsvector[i], corrsvector[i],
corrsvector[i], 1, corrsvector[i],
corrsvector[i], corrsvector[i], 1),
3,3)
# Sample from multivariate discrete distribution and generate data.frame
sims[[i]] <- ordsample(n, list(dist[[i]], dist[[i]], dist[[i]]), R[[i]])
}
proc.time() - ptm
Rprof(NULL)
summaryRprof("file.out")
# Compute variation measure d^2
# Calculated on the first column of the simulate data matrix - but is similar for other columns with large samples
# Only the cumulative probabilites for the first distribution are calculated
sims.dsquare <- lapply(sims, function(x){
sum((cumsum(prop.table(table(x[,1])))[-6] - 1/2)^2)
})
# Compute sd from samples - Mean across three distributions
sims.sd <- lapply(sims, function(x){
(sd(x[,1])+sd(x[,2])+sd(x[,3]))/3
})
# Avopscore Mean and Variance
sims.avopscore <- lapply(sims,rowMeans)
sims.avopscore.Mean <- lapply(sims.avopscore,mean)
sims.avopscore.Var <- lapply(sims.avopscore,var)
# Av. correlation
sims.cormatrices <- lapply(sims, cor)
sims.cormatrices.lower  <- lapply(sims.cormatrices, function(x) {x[lower.tri(x)]})
sims.av.correlation <- lapply(sims.cormatrices.lower, mean)
# Av. variances
sims.colvars  <- lapply(sims, function(x) {apply(x, 2, var)})
sims.av.variance  <- lapply(sims.colvars, mean)
# Av. kurtosis
sims.colkkurtosis  <- lapply(sims, function(x) {apply(x, 2, kurtosis)})
sims.av.kurtosis  <- lapply(sims.colkkurtosis, mean)
# Av. IQR
sims.col.IQR  <- lapply(sims, function(x) {apply(x, 2, IQR)})
sims.av.IQR  <- lapply(sims.col.IQR, mean)
# COMBING IN DATA FRAME
values <- data.frame(
corr = unlist(sims.av.correlation),
sd = unlist(sims.sd),
dsquare = unlist(sims.dsquare),
var.opscore = unlist(sims.avopscore.Var),
mean.opscore = unlist(sims.avopscore.Mean),
av.iqr = unlist(sims.av.IQR),
av.kurtosis = unlist(sims.av.kurtosis),
av.variance = unlist(sims.av.variance))
# Max dsquared value = (6-1)/6 = (categories k - 1)/4
values$lsquare <- values$dsquare/(5-1)/4
values$normalized.dsquare <- values$dsquare/max(values$dsquare)
summary(values$dsquare)
summary(values$lsquare)
summary(values$normalized.dsquare)
values <- arrange(values, dsquare, sd)
write.table(values, "simulatedvalues.csv", sep=",")
install.packages("fulltext")
library(citations)
install.packages("roxygen2")
library(citations)
install.packages("testthat")
library(citations)
library(citations)
