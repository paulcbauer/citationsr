n.docs <- length(file.paths)
# Specify number of documents
if(!is.null(number)){n.docs <- number}
# Load texts, clean them and save them
for (i in 1:n.docs){
con <- file(file.paths[i], encoding = "UTF-8") #
x <- readLines(con)
close(con)
x <- paste(x, collapse = " ")
# REPLACE DOTS IN DIFFERENT FORMS
# ABBREVIATIONS
x <- stringr::str_replace_all(x, "et al\\.", "AND OTHERS")
x <- stringr::str_replace_all(x, "e\\.g\\.", "FOR EXAMPLE")
x <- stringr::str_replace_all(x, "No\\.", "NUMBER")
x <- stringr::str_replace_all(x, "NO\\.", "NUMBER")
x <- stringr::str_replace_all(x, "fig\\.", "FIGURE")
x <- stringr::str_replace_all(x, "obs\\.", "OBSERVATIONS")
x <- stringr::str_replace_all(x, "var\\. ", "VARIANCE")
x <- stringr::str_replace_all(x, "i\\.e\\.", "THAT IS")
x <- stringr::str_replace_all(x, "vs\\.", "VERSUS")
x <- stringr::str_replace_all(x, " p\\.", "PAGE")
x <- stringr::str_replace_all(x, "pp\\.", "PAGE")
x <- stringr::str_replace_all(x, " pp\\.", "PAGE")
x <- stringr::str_replace_all(x, "\\(pp\\.", "PAGE")
x <- stringr::str_replace_all(x, "apps\\.", "APPENDIX")
x <- stringr::str_replace_all(x, "app\\.", "APPENDIX")
x <- stringr::str_replace_all(x, "\\. \\. \\.", "\\[;;;\\]")
x <- stringr::str_replace_all(x, "n\\.d\\.", "NO DATE")
x <- stringr::str_replace_all(x, "f\\.n\\.", "FOOTNOTE")
x <- stringr::str_replace_all(x, "VOL\\.", "VOLUME")
x <- stringr::str_replace_all(x, "vol\\.", "VOLUME")
x <- stringr::str_replace_all(x, "esp\\.", "ESPECIALLY")
x <- stringr::str_replace_all(x, "Jr\\.", "JUNIOR")
x <- stringr::str_replace_all(x, "U\\.S\\.", "UNITED STATES")
x <- stringr::str_replace_all(x, "U\\.S\\.", "UNITED STATES")
# REPLACE DOTS IN DECIMAL NUMBER WITH ","
detected <- unlist(stringr::str_extract_all(x, "[0-9]{1,10}\\.[0-9]{1,10}"))
if(length(detected)>0){
for(z in 1:length(detected)){
x <- stringr::str_replace(x, detected[z], stringr::str_replace_all(detected[z], "\\.", ","))
}
}
# NAMES IN TEXT
x <- stringr::str_replace_all(x, "G. Bingham Powell and Guy Whitten", "Bingham Powell and Whitten")
# Replace citation of interest with original name (NOT NECESSARY)
# x <- stringr::str_replace_all(x, "Glaeser AND OTHERS", "Glaeser et al.")
# 0.82 -> 0,82
# Get rid of em-dashes - QUESTION WHICH COMPUTER YOU USE
# x <- iconv(x, "", "ASCII", "byte")
# x <- stringr::str_replace_all(x, "<c2><ad><e2><80><93>", "-")
# x <- iconv(x, "", "UTF-8")
fileConn<-file(file.paths[i])
writeLines(x, fileConn, useBytes = TRUE)
close(fileConn)
# Counter
if(stringr::str_detect(as.character(i), "[0-9]*0")){cat(i, ".. ", sep="")}
}
cat("\n\n", n.docs, " texts/documents have been cleaned in folder '", folder ,"' and are now reeeeaaadddyyy to be analyzed!\n\n", sep = "")
}
setwd("C:/Users/pbauer/Google Drive/Research/2016_Quality_of_citations/data")
extract_text(folder = "Beck 1995")
delete_reference_section(folder = "Beck 1995")
delete_running_heads(folder = "Beck 1995")
clean_text(folder = "Beck 1995")
extract_citation_cases(folder = "Beck 1995",
authorname = "Beck, Katz",
studyyear = "1995"
) # scope = 2
setwd("C:/Users/pbauer/Google Drive/Research/2016_Quality_of_citations/data")
install.packages("quanteda")
install.packages("stm")
install.packages("qdap")
require(quanteda, quietly=TRUE, warn.conflicts = FALSE)
require(stm, quietly=TRUE, warn.conflicts = FALSE)
require(ggplot2, quietly=TRUE)
require(scales)
require(dplyr, warn.conflicts = FALSE)
require(stringr)
require(knitr)
options(stringsAsFactors=F)
extract_citation_cases <- function(folder, authorname, studyyear, scope=NULL, number=NULL){
#############################################
### FUNCTION TO GENERATE THE SEARCH TERMS ###
#############################################
authorname <- unlist(stringr::str_split(authorname, ","))
authorname <- gsub(" ", "", authorname)
length.authorname <- length(authorname)
if(length.authorname==1){
searchterms <- paste(authorname[1], "(|\\'s|\\’s|\\’|\\')(|,)\\s{0,2}(|\\[|\\()" ,studyyear, "", sep="")
}
# stringr::str_extract("sdflkjds Beck & Katz, 1995 sljsdf", searchterms)
if(length.authorname==2){
searchterms <- paste(authorname[1], " (\\&|and) ", authorname[2], "(|\\'s|\\’s|\\’|\\')(|,)\\s{0,2}(|\\[|\\()" ,studyyear, "(\\s{0,2}(:|,)(?# Komma oder Doppelpunkt)\\s{0,2}(PAGE|)(?# Page kommt vor oder nicht)(\\s{0,2}|)(?# nochmal space oder nicht)\\d*(?# zahl mit länge 0 oder mehr)(\\]|\\)|)(?# schliesst mit versch klammer oder nicht)|)(?# seitenzahlen ja,nein, falls nein einfach klammer matchen)(\\]|\\)|)", sep="")
# paste(authorname[1], " and ", authorname[2], sep=""), # Without year!
}
if(length.authorname==3){
searchterms <- c(paste(authorname[1], ", ", authorname[2], ", & ", authorname[3], " ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", & ", authorname[3], ", ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", and ", authorname[3], ", ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", and ", authorname[3], " ", studyyear, sep=""),
paste(authorname[1], ", ", authorname[2], ", and ", authorname[3], " ", "\\(" ,studyyear, "\\)", sep=""),
paste(authorname[1], " AND OTHERS, ",studyyear, sep=""),
paste(authorname[1], " AND OTHERS ",studyyear, sep=""),
paste(authorname[1], " AND OTHERS (" ,studyyear, ")", sep=""),
paste(authorname[1], " AND OTHERS, (" ,studyyear, ")", sep="")
)
}
if(length.authorname>=3){
searchterms <- c(
paste(authorname[1], " AND OTHERS", "(|,)\\s{0,2}(|\\[|\\()" ,studyyear, "(\\s{0,2}(:|,)(?# Komma oder Doppelpunkt)\\s{0,2}(PAGE|)(?# Page kommt vor oder nicht)(\\s{0,2}|)(?# nochmal space oder nicht)\\d*(?# zahl mit länge 0 oder mehr)(\\]|\\)|)(?# schliesst mit versch klammer oder nicht)|)(?# seitenzahlen ja,nein, falls nein einfach klammer matchen)(\\]|\\)|)", sep="")
)
}
#############################################
# Get file names
file.names <- dir(paste("./", folder, sep = ""), pattern = ".txt")
file.paths <- paste(paste("./", folder, "/", sep = ""), file.names, sep="")
n.docs <- length(file.paths)
# Specify number of documents
if(!is.null(number)){n.docs <- number}
# Regexp for search terms and dependency on scope
searchterms <- paste("\\.[^.]*", searchterms, "[^.]*\\.", sep = "")
if(!is.null(scope)){
searchterms <- paste(paste(rep("\\.[^.]*", scope), collapse=""),
searchterms,
paste(rep("[^.]*\\.", scope), collapse=""),
sep = "")
}
#
# "\\. = Starts with a dot
# [^.] = not dot characters
# * = 0 or more
# \\." = Ends with a dot
# Load documents and search for full citation in them
all.docs.cit.cases <- NULL
for (i in 1:n.docs){
con <- file(file.paths[i], encoding = "UTF-8")
x <- readLines(con)
close(con)
cit.cases.doc.i <- stringr::str_extract_all(x, paste(searchterms, collapse="|"))
# identify ". Beck and Katz 1995."
# cit.cases.doc.i[cit.cases.doc.i==". Beck and Katz 1995."]
all.docs.cit.cases[i] <- cit.cases.doc.i
# Counter
if(stringr::str_detect(as.character(i), "[0-9]*0")){cat(i, ".. ", sep="")}
}
# Get fist estimate of number of citation cases
total.citation.cases <- sum(sapply(all.docs.cit.cases, length))
cat("\n For ", authorname, " we have identified ", total.citation.cases, " citation cases within ", n.docs, " documents.", sep="")
# Generate dataframe with citation cases
citation.data <- data.frame(document = 1:total.citation.cases, citation.case = 1:total.citation.cases)
citation.data[,1] <- rep(file.names, sapply(all.docs.cit.cases, length))
citation.data[,2] <- unlist(all.docs.cit.cases)
# Change document names
citation.data$document <- sub("\\s+$", "", stringr::str_extract(citation.data$document, "^[^-]+"))
# Save citation cases in table (csv and html)
write.table(citation.data, file =  paste("./", folder, "_citation_cases.csv", sep = ""), sep=",")
print(xtable::xtable(citation.data),type='html',comment=FALSE, file=paste("./", folder, "_citation_cases.html", sep = ""))
cat("\n \nThey are printed and saved as files in the working directory: '*_citation_cases.csv' and '*_citation_cases.html'.\n\n")
}
extract_citation_cases(folder = "Beck 1995",
authorname = "Beck, Katz",
studyyear = "1995"
) # sc
install.packages("knitr")
install.packages("knitr")
require(knitr)
tf <- read.csv('C:/Users/pbauer/Google Drive/Research/2016_Quality_of_citations/data/Beck 1995_citation_cases.csv')
tf$citation.case <- iconv(tf$citation.case, from='UTF-8', to='latin1', sub="")
tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
tf <- tf[!is.na(tf$year),] # deleting citations with empty years
tf <- tf[!duplicated(tf$citation.case),] # deleting duplicates
kable(tf[0:9,], format = 'markdown')
tf$num <- 1 # there must be another way...
x <- aggregate(tf[,"num"], by=list(tf$document), FUN=sum)
# table(x$x)
range.x <- range(x$x)
breaks <- c(range.x[1]-1.5, range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(0, seq(range.x[1], range.x[2], 1))
hist(x$x, xlab="N citation cases per document", main = "Citation cases: Beck & Katz 1995", xaxt="n", , breaks=breaks, cex.main=1, ylab="Frequency = N documents") # ylim=c(0,700)
axis(1,seq.x)
tf$citation_counts <- str_count(tf$citation.case, "(19|20)[0-9]{2}") # there were 23 0-cases so I took out \b but there are still 5..
# table(tf$citation_counts)
tf$citation.case[tf$citation_counts == 7]
tf$citation.case[tf$citation_counts == 6]
tf$citation.case[tf$citation_counts == 2][1:10]
x <- tf$citation_counts
# table(x)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(seq(range.x[1], range.x[2], 1))
hist(x, xlab="N references within citation case", main = "Citation cases: Beck & Katz 1995", xaxt="n", breaks = breaks,  cex.main=1, ylab="Frequency = N citation cases")
axis(1,seq.x)
by_years <- group_by(tf, year)
tf_group <- summarize(by_years, mean_citations = mean(citation_counts, na.rm = TRUE))
p <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_citations))
p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Average number of references in citation case")
tf$num <- 1 # there must be another way...
x <- aggregate(tf[,"num"], by=list(tf$document), FUN=sum)
# table(x$x)
range.x <- range(x$x)
breaks <- c(range.x[1]-1.5, range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(0, seq(range.x[1], range.x[2], 1))
hist(x$x, xlab="N citation cases per document", main = "Citation cases: Beck & Katz 1995", xaxt="n", , breaks=breaks, cex.main=1, ylab="Frequency = N documents") # ylim=c(0,700)
axis(1,seq.x)
```{r, eval=T, echo=F, warning=F}
tf <- read.csv('C:/Users/pbauer/Google Drive/Research/2016_Quality_of_citations/data/Beck 1995_citation_cases.csv')
tf$citation.case <- iconv(tf$citation.case, from='UTF-8', to='latin1', sub="")
tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
tf <- tf[!is.na(tf$year),] # deleting citations with empty years
tf <- tf[!duplicated(tf$citation.case),] # deleting duplicates
kable(tf[0:9,], format = 'markdown')
tf$num <- 1 # there must be another way...
x <- aggregate(tf[,"num"], by=list(tf$document), FUN=sum)
# table(x$x)
range.x <- range(x$x)
breaks <- c(range.x[1]-1.5, range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(0, seq(range.x[1], range.x[2], 1))
hist(x$x, xlab="N citation cases per document", main = "Citation cases: Beck & Katz 1995", xaxt="n", , breaks=breaks, cex.main=1, ylab="Frequency = N documents") # ylim=c(0,700)
axis(1,seq.x)
tf$citation_counts <- str_count(tf$citation.case, "(19|20)[0-9]{2}") # there were 23 0-cases so I took out \b but there are still 5..
tf$citation.case[tf$citation_counts == 7]
tf$citation.case[tf$citation_counts == 6]
tf$citation.case[tf$citation_counts == 2][1:10]
x <- tf$citation_counts
# table(x)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(seq(range.x[1], range.x[2], 1))
hist(x, xlab="N references within citation case", main = "Citation cases: Beck & Katz 1995", xaxt="n", breaks = breaks,  cex.main=1, ylab="Frequency = N citation cases")
axis(1,seq.x)
x
tf$citation_counts <- str_count(tf$citation.case, "(19|20)[0-9]{2}") # there were 23 0-cases so I took out \b but there are still 5..
library(stringr)
?str_count
library(quanteda, quietly=TRUE, warn.conflicts = FALSE)
library(stm, quietly=TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly=TRUE)
library(scales)
library(dplyr, warn.conflicts = FALSE)
library(stringr)
library(knitr)
options(stringsAsFactors=F)
tf <- read.csv('C:/Users/pbauer/Google Drive/Research/2016_Quality_of_citations/data/Beck 1995_citation_cases.csv')
tf$citation.case <- iconv(tf$citation.case, from='UTF-8', to='latin1', sub="")
tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
tf <- tf[!is.na(tf$year),] # deleting citations with empty years
tf <- tf[!duplicated(tf$citation.case),] # deleting duplicates
kable(tf[0:9,], format = 'markdown')
tf$num <- 1 # there must be another way...
x <- aggregate(tf[,"num"], by=list(tf$document), FUN=sum)
# table(x$x)
range.x <- range(x$x)
breaks <- c(range.x[1]-1.5, range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(0, seq(range.x[1], range.x[2], 1))
hist(x$x, xlab="N citation cases per document", main = "Citation cases: Beck & Katz 1995", xaxt="n", , breaks=breaks, cex.main=1, ylab="Frequency = N documents") # ylim=c(0,700)
axis(1,seq.x)
tf$citation_counts <- str_count(tf$citation.case, "(19|20)[0-9]{2}") # there were 23 0-cases so I took out \b but there are still 5..
tf$citation.case[tf$citation_counts == 7]
tf$citation.case[tf$citation_counts == 6]
tf$citation.case[tf$citation_counts == 2][1:10]
x <- tf$citation_counts
# table(x)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(seq(range.x[1], range.x[2], 1))
hist(x, xlab="N references within citation case", main = "Citation cases: Beck & Katz 1995", xaxt="n", breaks = breaks,  cex.main=1, ylab="Frequency = N citation cases")
axis(1,seq.x)
by_years <- group_by(tf, year)
tf_group <- summarize(by_years, mean_citations = mean(citation_counts, na.rm = TRUE))
p <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_citations))
p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Average number of references in citation case")
par(mfrow=c(1,2))
x <- nchar(tf$citation.case)
# table(x)
hist(x, xlab="Characters per citation case", main = "\n", breaks = 20,  cex.main=1, ylab="Frequency = N citation cases")
x <- sapply(str_extract_all(tf$citation.case, "\\W+"), length) # Very rough count of words
hist(x, xlab="Words per citation case", main = "\n", breaks = 20,  cex.main=1, ylab="Frequency = N citation cases")
tf$signal_example <- str_detect(tf$citation.case, "([Ff]or example)|([Ff]or instance)|([Ee]\\.g\\.)")
signal.words <- "follow|recommend|validat|suggest|accordance|advice|demonstrate|confirm|support|in line with|based"
tf$signal_positive <- str_detect(tf$citation.case, signal.words)
by_years <- group_by(tf, year)
tf_group <- summarize(by_years, mean_signal_example = mean(signal_example, na.rm = TRUE), mean_signal_positive = mean(signal_positive, na.rm = TRUE))
p <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_signal_positive))
p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Average number of citations with `positive' signal")
authors <- tokenize(toLower(tf$document), removePunct=TRUE, removeNumbers=TRUE)
authors <- unique(unlist(authors))
tf$year <- gsub('.*([0-9]{4}).*', tf$document, repl='\\1')
# tokenizing
tokens <- tokenize(toLower(tf$citation.case), removePunct=TRUE, removeNumbers=TRUE)
# removing stopwords, author names, and other frequent words
tokens <- removeFeatures(tokens,
c(stopwords("english"), "other", "others", "see", "also", authors))
# stemming?
#tokens <- lapply(tokens, wordstem)
# creating n-grams
ngrams <- lapply(tokens, ngrams, 1:3)
# putting it all back together...
ngrams <- unlist(lapply(ngrams, paste, collapse=" "))
# deleting empty citations...
todelete <- which(ngrams=="")
tf <- tf[-todelete,]
tokens <- tokens[-todelete]
ngrams <- ngrams[-todelete]
cit <- corpus(ngrams)
docnames(cit) <- paste0(1:nrow(tf), '_', tf$document)
# summary(cit)
citmat <- dfm(cit)
plot(citmat, rot.per=0, scale=c(3.5, .3), max.words=50)
par(mfrow=c(1,1))
plot(citmat, rot.per=0, scale=c(3.5, .3), max.words=50)
citstm <- convert(citmat, to="stm")
K <- 3:6
out <- prepDocuments(citstm$documents, citstm$vocab,
tf[,c("year", "document", "citation.case")])
manymodels <- manyTopics(out$documents, out$vocab, K=K, verbose=TRUE,
LDAbeta=FALSE, sigma.prior=0.5, seed=777, runs=2, max.em.its=10, net.max.em.its=2) # , runs=5
chosen <- which.max(unlist(lapply(manymodels$semcoh, mean)))
#cat(K[chosen], 'number of topics is the best fit!')
model <- manymodels$out[[chosen]]
# topic proportions
words <- labelTopics(model, n=20)
topictab <- data.frame(
"Topic" = 1:K[chosen],
"Words" = apply(words$lift, 1, paste, collapse=", "))
kable(topictab, format = 'markdown')
# most representative documents for each topic
#findThoughts(model, out$meta$document)
# now showing also the text of the documents
df <- findThoughts(model, paste0(out$meta$document, ': ', out$meta$citation.case), n=1)
topictab <- data.frame(
"Topic" = 1:K[chosen],
"Representative document" = unlist(df$docs))
names(topictab)[2] <- "Representative document"
kable(topictab, format='markdown')
topics <- data.frame(
prop = c(model$theta),
document = rep(1:nrow(out$meta), times=K[chosen]),
topic = rep(1:K[chosen], each=nrow(out$meta)),
year = rep(out$meta$year, times=K[chosen]))
agg <- aggregate(topics$prop,
by=list(topic=factor(topics$topic), year=topics$year),
FUN=mean)
names(agg)[names(agg)=="x"] <- "prop"
library(ggplot2)
library(scales)
p <- ggplot(agg, aes(x=year, y=prop, group=topic, fill=topic))
pq <- p + geom_area(position="stack") + theme_minimal() +
theme(axis.ticks.y=element_blank(), axis.title=element_blank(),
axis.text.y=element_blank())
pq
p <- ggplot(agg, aes(x=year, y=prop, group=topic, color=topic))
pq <- p + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Average topic proportion", labels=percent)
pq
kable(topictab, format='markdown')
require(quanteda, quietly=TRUE, warn.conflicts = FALSE)
require(stm, quietly=TRUE, warn.conflicts = FALSE)
require(ggplot2, quietly=TRUE)
require(scales)
require(dplyr, warn.conflicts = FALSE)
require(stringr)
require(knitr)
options(stringsAsFactors=F)
kable(topictab, format='markdown')
library(knitr)
library(stm)
R.Version
R.Version()
require(quanteda, quietly=TRUE, warn.conflicts = FALSE)
require(stm, quietly=TRUE, warn.conflicts = FALSE)
require(ggplot2, quietly=TRUE)
require(scales)
require(dplyr, warn.conflicts = FALSE)
require(stringr)
require(knitr)
options(stringsAsFactors=F)
tf <- read.csv('C:/Users/pbauer/Google Drive/Research/2016_Quality_of_citations/data/Beck 1995_citation_cases.csv')
tf$citation.case <- iconv(tf$citation.case, from='UTF-8', to='latin1', sub="")
tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
tf <- tf[!is.na(tf$year),] # deleting citations with empty years
tf <- tf[!duplicated(tf$citation.case),] # deleting duplicates
kable(tf[0:9,], format = 'markdown')
tf$num <- 1 # there must be another way...
x <- aggregate(tf[,"num"], by=list(tf$document), FUN=sum)
# table(x$x)
range.x <- range(x$x)
breaks <- c(range.x[1]-1.5, range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(0, seq(range.x[1], range.x[2], 1))
hist(x$x, xlab="N citation cases per document", main = "Citation cases: Beck & Katz 1995", xaxt="n", , breaks=breaks, cex.main=1, ylab="Frequency = N documents") # ylim=c(0,700)
axis(1,seq.x)
tf$citation_counts <- str_count(tf$citation.case, "(19|20)[0-9]{2}") # there were 23 0-cases so I took out \b but there are still 5..
tf$citation.case[tf$citation_counts == 7]
tf$citation.case[tf$citation_counts == 6]
tf$citation.case[tf$citation_counts == 2][1:10]
x <- tf$citation_counts
# table(x)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(seq(range.x[1], range.x[2], 1))
hist(x, xlab="N references within citation case", main = "Citation cases: Beck & Katz 1995", xaxt="n", breaks = breaks,  cex.main=1, ylab="Frequency = N citation cases")
axis(1,seq.x)
by_years <- group_by(tf, year)
tf_group <- summarize(by_years, mean_citations = mean(citation_counts, na.rm = TRUE))
p <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_citations))
p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Average number of references in citation case")
par(mfrow=c(1,2))
x <- nchar(tf$citation.case)
# table(x)
hist(x, xlab="Characters per citation case", main = "\n", breaks = 20,  cex.main=1, ylab="Frequency = N citation cases")
x <- sapply(str_extract_all(tf$citation.case, "\\W+"), length) # Very rough count of words
hist(x, xlab="Words per citation case", main = "\n", breaks = 20,  cex.main=1, ylab="Frequency = N citation cases")
tf$signal_example <- str_detect(tf$citation.case, "([Ff]or example)|([Ff]or instance)|([Ee]\\.g\\.)")
signal.words <- "follow|recommend|validat|suggest|accordance|advice|demonstrate|confirm|support|in line with|based"
tf$signal_positive <- str_detect(tf$citation.case, signal.words)
by_years <- group_by(tf, year)
tf_group <- summarize(by_years, mean_signal_example = mean(signal_example, na.rm = TRUE), mean_signal_positive = mean(signal_positive, na.rm = TRUE))
p <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_signal_positive))
p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Average number of citations with `positive' signal")
authors <- tokenize(toLower(tf$document), removePunct=TRUE, removeNumbers=TRUE)
authors <- unique(unlist(authors))
tf$year <- gsub('.*([0-9]{4}).*', tf$document, repl='\\1')
# tokenizing
tokens <- tokenize(toLower(tf$citation.case), removePunct=TRUE, removeNumbers=TRUE)
# removing stopwords, author names, and other frequent words
tokens <- removeFeatures(tokens,
c(stopwords("english"), "other", "others", "see", "also", authors))
# stemming?
#tokens <- lapply(tokens, wordstem)
# creating n-grams
ngrams <- lapply(tokens, ngrams, 1:3)
# putting it all back together...
ngrams <- unlist(lapply(ngrams, paste, collapse=" "))
# deleting empty citations...
todelete <- which(ngrams=="")
tf <- tf[-todelete,]
tokens <- tokens[-todelete]
ngrams <- ngrams[-todelete]
cit <- corpus(ngrams)
docnames(cit) <- paste0(1:nrow(tf), '_', tf$document)
# summary(cit)
citmat <- dfm(cit)
par(mfrow=c(1,1))
plot(citmat, rot.per=0, scale=c(3.5, .3), max.words=50)
citstm <- convert(citmat, to="stm")
K <- 3:6
out <- prepDocuments(citstm$documents, citstm$vocab,
tf[,c("year", "document", "citation.case")])
manymodels <- manyTopics(out$documents, out$vocab, K=K, verbose=TRUE,
LDAbeta=FALSE, sigma.prior=0.5, seed=777, runs=2, max.em.its=10, net.max.em.its=2) # , runs=5
# finding right number of topics
chosen <- which.max(unlist(lapply(manymodels$semcoh, mean)))
#cat(K[chosen], 'number of topics is the best fit!')
model <- manymodels$out[[chosen]]
# topic proportions
#apply(model$theta, 2, mean)
# finding right number of topics
chosen <- which.max(unlist(lapply(manymodels$semcoh, mean)))
#cat(K[chosen], 'number of topics is the best fit!')
model <- manymodels$out[[chosen]]
# topic proportions
#apply(model$theta, 2, mean)
words <- labelTopics(model, n=20)
topictab <- data.frame(
"Topic" = 1:K[chosen],
"Words" = apply(words$lift, 1, paste, collapse=", "))
kable(topictab, format = 'markdown')
# most representative documents for each topic
#findThoughts(model, out$meta$document)
# now showing also the text of the documents
df <- findThoughts(model, paste0(out$meta$document, ': ', out$meta$citation.case), n=1)
topictab <- data.frame(
"Topic" = 1:K[chosen],
"Representative document" = unlist(df$docs))
names(topictab)[2] <- "Representative document"
kable(topictab, format='markdown')
install.packages(c("BH", "curl", "digest", "ggplot2", "git2r", "Hmisc", "htmltools", "jsonlite", "lubridate", "Matching", "R.cache", "R.utils", "Rcpp", "rmarkdown", "roxygen2", "rstudioapi", "stringdist"))
install.packages(c("mgcv", "nlme"), lib="G:/My Documents/R/R-3.2.3/library")
install.packages(c("BH", "curl", "digest", "ggplot2", "git2r",
)
library("knitr", lib.loc="~/R/win-library/3.2")
install.packages(c("BH", "curl", "digest", "ggplot2", "git2r", "Hmisc", "htmltools", "jsonlite", "lubridate", "Matching", "R.cache", "R.utils", "Rcpp", "rmarkdown", "roxygen2", "rstudioapi", "stringdist"))
library("knitr", lib.loc="~/R/win-library/3.2")
# install.packagkes('digest')
install.packagkes('digest')
install.packages('digest')
install.packages("digest")
require(quanteda, quietly=TRUE, warn.conflicts = FALSE)
install.packages("Rcpp")
install.packages("quanteda")
install.packages("stm")
install.packages("knitr")
install.packages('digest')
install.packages("digest")
install.packages("digest")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
?require
library(citations)
