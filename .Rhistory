tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
message("Warning: ", sum(is.na(tf$year)), " citation cases with missing year will be excluded from analysis.")
todelete <- which(is.na(tf$year))
message("Warning: ", sum(duplicated(tf$citation.case)), " duplicated citation cases will be excluded from analysis.")
todelete <- c(todelete, which(duplicated(tf$citation.case)))
message("Warning: ", sum(nchar(tf$citation.case)>1000), " citation cases longer than 1000 characters will be excluded from analysis.")
todelete <- unique(c(todelete, which(nchar(tf$citation.case)>1000)))
# exporting
write.csv(tf[todelete,], file=paste0(output, '/parsing-errors.csv'), row.names=FALSE)
tf <- tf[-todelete,]
message("A total of ", nrow(tf), " citation cases will be included in the analysis.")
# generating histogram with times cited within document
x <- table(tf$document)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- seq(range.x[1], range.x[2], 1)
f1 <- paste0(output, '/01-times-cited-within-document.pdf')
pdf(f1, height=4, width=6)
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.025)
hist(x, xlab="N citation cases per document",
main = paste0("Citation cases: ", article), xaxt="n",
breaks=breaks, cex.main=1, ylab="Frequency = N documents")
axis(1,seq.x)
dev.off()
message("File generated: ", f1)
# generating histograms for co-citations
tf$citation_counts <- stringr::str_count(tf$citation.case, "(19|20)[0-9]{2}")
x <- tf$citation_counts
# table(x)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(seq(range.x[1], range.x[2], 1))
f2 <- paste0(output, '/02-co-citations-in-citation-case.pdf')
pdf(f2, height=4, width=6)
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.025)
hist(x, xlab="N references within citation case",
main = paste0("Citation cases: ", article), xaxt="n", breaks = breaks,  cex.main=1, ylab="Frequency = N citation cases")
axis(1,seq.x)
dev.off()
message("File generated: ", f2)
# generating average number of references per citation over time
tf_group <- aggregate(tf$citation_counts, by=list(year=tf$year), FUN=mean, na.rm=TRUE)
p <- ggplot(tf_group, aes(x=as.numeric(year), y=x))
pq <- p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Average number of references in citation case") +
ggtitle(paste0("Citation cases: ", article))
f3 <- paste0(output, '/03-co-citations-over-time.pdf')
ggsave(pq, file=f3, height=4, width=6)
message("File generated: ", f3)
# figure with positive signals
signal.words <- paste0("follow|recommend|validate|suggest|accordance|advice|demonstrate",
"|confirm|support|in line with|based")
tf$signal_positive <- grepl(signal.words, tf$citation.case)
tf_group <- aggregate(tf$signal_positive, by=list(year=tf$year), FUN=mean, na.rm=TRUE)
p <- ggplot(tf_group, aes(x=as.numeric(year), y=x))
pq <- p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Proportion of citations with `positive' signal",
label=percent) + ggtitle(paste0("Citation cases: ", article))
f4 <- paste0(output, '/04-citations-with-positive-signal.pdf')
ggsave(pq, file=f4, height=4, width=6)
message("File generated: ", f4)
# text cleaning
authors <- tokenize(toLower(c(tf$document, article)), removePunct=T, removeNumbers=T)
authors <- unique(unlist(authors))
# tokenizing
tokens <- tokenize(toLower(tf$citation.case), removePunct=T, removeNumbers=T)
# removing stopwords, author names, and other frequent words
tokens <- removeFeatures(tokens,
c(stopwords("english"), "other", "others", "see", "also", "u", authors))
# stemming?
#tokens <- lapply(tokens, wordstem)
# creating n-grams
ngrams <- lapply(tokens, ngrams, 1:3)
# putting it all back together...
ngrams <- unlist(lapply(ngrams, paste, collapse=" "))
# constructing the DFM
cit <- corpus(ngrams)
docnames(cit) <- paste0(1:nrow(tf), '_', tf$document)
# summary(cit)
citmat <- dfm(cit)
# word cloud
f5 <- paste0(output, '/05-citations-word-cloud.pdf')
pdf(f5, height=4, width=4)
par (mar=c(0,0,0,0))
plot(citmat, rot.per=0, scale=c(3, .3), max.words=80)
dev.off()
message("File generated: ", f5)
# sentiment analysis
dict <- qdapDictionaries::key.pol
mydict <- dictionary(list(negative = dict$x[dict$y==-1],
postive = dict$x[dict$y==1]))
myDfm <- dfm(cit, dictionary = mydict)
tf$neg <- as.numeric(myDfm[,1])
tf$pos <- as.numeric(myDfm[,2])
tf$score <- (tf$pos - tf$neg)
tf_group <- aggregate(tf$score, by=list(year=tf$year), FUN=mean, na.rm=TRUE)
p <- ggplot(tf_group, aes(x=as.numeric(year), y=x))
pq <- p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Average sentiment in citations") +
ggtitle(paste0("Citation cases: ", article))
f6 <- paste0(output, '/06-sentiment-over-time.pdf')
ggsave(pq, file=f6, height=4, width=6)
message("File generated: ", f6)
}
analyze_citations(file, article, output)
file <- "C:/Users/pbauer/Google Drive/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
analyze_citations(file, article, output)
file <- "C:/Users/pbauer/Google Drive/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
text <- scan(file, what="character", sep="\n")
file <- "C:/Users/pbauer/Google Drive/Research/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
analyze_citations(file, article, output)
require(ggplot2)
require(scales)
require(quanteda)
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1")
file <- "~/Google Drive/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "fearon_2003"
analyze_citations(file, article, output)
library(citations)
analyze_citations(file, article, output)
file <- "C:/Users/paul/Google Drive/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
analyze_citations(file, article, output)
file <- "C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
analyze_citations(file, article, output)
file <- "C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "fearon_2003"
analyze_citations(file, article, output)
require(ggplot2)
require(scales)
require(quanteda)
# precleaning file
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1")
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1", sep=",")
library(citations)
library(citations)
file <- "C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "fearon_2003"
analyze_citations(file, article, output)
require(ggplot2)
require(scales)
require(quanteda)
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1", sep=",")
?read.csv2
?read.csv
??read.csv
tf <- read.table(tmp, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1", sep=",")
tmp
require(ggplot2)
require(scales)
require(quanteda)
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
text
tf <- read.table(tmp, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1", sep=",")
tf <- read.table(file, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1", sep=",")
tf <- read.csv(file, stringsAsFactors=F, row.names=NULL, fileEncoding="latin1", sep=",")
file <- "C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "fearon_2003"
analyze_citations(file, article, output)
require(ggplot2)
require(scales)
require(quanteda)
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, sep=",")
fix(tf)
library(citations)
?read.csv
file <- "C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "fearon_2003"
analyze_citations(file, article, output)
require(ggplot2)
require(scales)
require(quanteda)
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, sep=",")
tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
message("Warning: ", sum(is.na(tf$year)), " citation cases with missing year will be excluded from analysis.")
todelete <- which(is.na(tf$year))
message("Warning: ", sum(duplicated(tf$citation.case)), " duplicated citation cases will be excluded from analysis.")
todelete <- c(todelete, which(duplicated(tf$citation.case)))
message("Warning: ", sum(nchar(tf$citation.case)>1000), " citation cases longer than 1000 characters will be excluded from analysis.")
todelete <- unique(c(todelete, which(nchar(tf$citation.case)>1000)))
# exporting
write.csv(tf[todelete,], file=paste0(output, '/parsing-errors.csv'), row.names=FALSE)
getwd()
output
require(ggplot2)
require(scales)
require(quanteda)
# precleaning file
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
# reading file and cleaning data
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, sep=",")
# extracting year, deleting citations with empty years
tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
message("Warning: ", sum(is.na(tf$year)), " citation cases with missing year will be excluded from analysis.")
todelete <- which(is.na(tf$year))
message("Warning: ", sum(duplicated(tf$citation.case)), " duplicated citation cases will be excluded from analysis.")
todelete <- c(todelete, which(duplicated(tf$citation.case)))
message("Warning: ", sum(nchar(tf$citation.case)>1000), " citation cases longer than 1000 characters will be excluded from analysis.")
todelete <- unique(c(todelete, which(nchar(tf$citation.case)>1000)))
# exporting
tf <- tf[-todelete,]
message("A total of ", nrow(tf), " citation cases will be included in the analysis.")
x <- table(tf$document)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- seq(range.x[1], range.x[2], 1)
f1 <- paste0(output, '/01-times-cited-within-document.pdf')
pdf(f1, height=4, width=6)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data")
require(ggplot2)
require(scales)
require(quanteda)
# precleaning file
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, sep=",")
# extracting year, deleting citations with empty years
tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
message("Warning: ", sum(is.na(tf$year)), " citation cases with missing year will be excluded from analysis.")
todelete <- which(is.na(tf$year))
message("Warning: ", sum(duplicated(tf$citation.case)), " duplicated citation cases will be excluded from analysis.")
todelete <- c(todelete, which(duplicated(tf$citation.case)))
message("Warning: ", sum(nchar(tf$citation.case)>1000), " citation cases longer than 1000 characters will be excluded from analysis.")
todelete <- unique(c(todelete, which(nchar(tf$citation.case)>1000)))
# exporting
write.csv(tf[todelete,], file=paste0(output, '/parsing-errors.csv'), row.names=FALSE)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/output")
require(ggplot2)
require(scales)
require(quanteda)
text <- scan(file, what="character", sep="\n")
text <- gsub('\\\\"', "''", text)
text <- paste0(text, collapse="\n")
tmp <- tempfile()
writeLines(text, con=tmp)
tf <- read.csv(tmp, stringsAsFactors=F, row.names=NULL, sep=",")
# extracting year, deleting citations with empty years
tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
message("Warning: ", sum(is.na(tf$year)), " citation cases with missing year will be excluded from analysis.")
todelete <- which(is.na(tf$year))
message("Warning: ", sum(duplicated(tf$citation.case)), " duplicated citation cases will be excluded from analysis.")
todelete <- c(todelete, which(duplicated(tf$citation.case)))
message("Warning: ", sum(nchar(tf$citation.case)>1000), " citation cases longer than 1000 characters will be excluded from analysis.")
todelete <- unique(c(todelete, which(nchar(tf$citation.case)>1000)))
# exporting
# write.csv(tf[todelete,], file=paste0(output, '/parsing-errors.csv'), row.names=FALSE)
tf <- tf[-todelete,]
message("A total of ", nrow(tf), " citation cases will be included in the analysis.")
x <- table(tf$document)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- seq(range.x[1], range.x[2], 1)
f1 <- paste0(output, '/01-times-cited-within-document.pdf')
pdf(f1, height=4, width=6)
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.025)
hist(x, xlab="N citation cases per document",
main = paste0("Citation cases: ", article), xaxt="n",
breaks=breaks, cex.main=1, ylab="Frequency = N documents")
axis(1,seq.x)
dev.off()
message("File generated: ", f1)
write.csv(tf[todelete,], file=paste0(output, '/parsing-errors.csv'), row.names=FALSE)
tf$citation_counts <- stringr::str_count(tf$citation.case, "(19|20)[0-9]{2}")
x <- tf$citation_counts
# table(x)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(seq(range.x[1], range.x[2], 1))
f2 <- paste0(output, '/02-co-citations-in-citation-case.pdf')
pdf(f2, height=4, width=6)
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.025)
hist(x, xlab="N references within citation case",
main = paste0("Citation cases: ", article), xaxt="n", breaks = breaks,  cex.main=1, ylab="Frequency = N citation cases")
axis(1,seq.x)
dev.off()
message("File generated: ", f2)
tf_group <- aggregate(tf$citation_counts, by=list(year=tf$year), FUN=mean, na.rm=TRUE)
p <- ggplot(tf_group, aes(x=as.numeric(year), y=x))
pq <- p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Average number of references in citation case") +
ggtitle(paste0("Citation cases: ", article))
f3 <- paste0(output, '/03-co-citations-over-time.pdf')
ggsave(pq, file=f3, height=4, width=6)
message("File generated: ", f3)
signal.words <- paste0("follow|recommend|validate|suggest|accordance|advice|demonstrate",
"|confirm|support|in line with|based")
tf$signal_positive <- grepl(signal.words, tf$citation.case)
tf_group <- aggregate(tf$signal_positive, by=list(year=tf$year), FUN=mean, na.rm=TRUE)
p <- ggplot(tf_group, aes(x=as.numeric(year), y=x))
pq <- p + geom_point() + geom_line() + theme_minimal() +
theme(axis.title.x=element_blank()) +
scale_y_continuous("Proportion of citations with `positive' signal",
label=percent) + ggtitle(paste0("Citation cases: ", article))
f4 <- paste0(output, '/04-citations-with-positive-signal.pdf')
ggsave(pq, file=f4, height=4, width=6)
message("File generated: ", f4)
authors <- tokenize(toLower(c(tf$document, article)), removePunct=T, removeNumbers=T)
authors <- unique(unlist(authors))
# tokenizing
tokens <- tokenize(toLower(tf$citation.case), removePunct=T, removeNumbers=T)
# removing stopwords, author names, and other frequent words
tokens <- removeFeatures(tokens,
c(stopwords("english"), "other", "others", "see", "also", "u", authors))
# stemming?
#tokens <- lapply(tokens, wordstem)
# creating n-grams
ngrams <- lapply(tokens, ngrams, 1:3)
# putting it all back together...
ngrams <- unlist(lapply(ngrams, paste, collapse=" "))
# constructing the DFM
cit <- corpus(ngrams)
docnames(cit) <- paste0(1:nrow(tf), '_', tf$document)
# summary(cit)
citmat <- dfm(cit)
f5 <- paste0(output, '/05-citations-word-cloud.pdf')
pdf(f5, height=4, width=4)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Title of my first plot")
plot(citmat, rot.per=0, scale=c(3, .3), max.words=80)
dev.off()
message("File generated: ", f5)
library(citations)
library(citations)
library(citations)
library(citations)
setwd("~/Google Drive/2016_Quality_of_citations/")
library(citations)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/")
library(citations)
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
# Audretsch 1996
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Beck 1995
dir.create("output/beck_1995")
file <- "data/Beck 1995_citation_cases.csv"
article <- "Beck and Katz (1995)"
output <- "output/beck_1995"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Fearon 2003
dir.create("output/fearon_2003")
file <- "data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "output/fearon_2003"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Inglehart 2000
dir.create("output/inglehart_2000")
file <- "data/Inglehart 2000_citation_cases.csv"
article <- "Inglehart and Baker (2000)"
output <- "output/inglehart_2000"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Uzzi 1996
dir.create("output/uzzi_1996")
file <- "data/Uzzi 1996_citation_cases.csv"
article <- "Uzzi (1996)"
output <- "output/uzzi_1996"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Acemoglu 2001
dir.create("output/acemoglu_2001")
file <- "data/Acemoglu 2001_citation_cases.csv"
article <- "Acemoglu, Johnson & Robinson (2001)"
output <- "output/acemoglu_2001"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
library(citations)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/")
library(citations)
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
library(citations)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/")
library(citations)
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
library(citations)
x <- "This is a test if it works as well? with the question mark. or not."
searchterms <- paste("(\\.[^.]|\\?[^?])*searchtermhere[^.]*\\.", sep = "")
stringr::str_extract_all(x, paste(searchterms, collapse="|"))
stringr::str_extract_all(x, searchterms)
x
searchterms <- paste("(\\.|\\?)[^.]*searchtermhere[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
x <- ".This is a test if it works as well? with the question mark. or not."
searchterms <- paste("(\\.|\\?)[^.]*searchtermhere[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
x <- ".This is a test if it works as well? with searchtermhere the question mark. or not."
searchterms <- paste("(\\.|\\?)[^.]*", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
searchterms <- paste("(\\.[^.]*|\\?[^?]*)", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
searchterms <- paste("(\\?[^?]*)", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
searchterms <- paste("(\\.[^.]*|\\?[^.]*)", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
searchterms <- paste("(\\?[^.]*)", "searchtermhere", "[^.]*\\.", sep = "")
stringr::str_extract_all(x, searchterms)
x <- "lksdjfl! sdflkj. sdfsdf? lskdjfldj. sldjdsl"
x <- stringr::str_replace_all(x, "\\?", "\\.\\?")
x
x <- stringr::str_replace_all(x, '\\\\"', "''")
x <- stringr::str_replace_all(x, "!", "\\.!")
x
library(citations)
library(citations)
setwd("C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/")
library(citations)
# Acemoglu 2001
dir.create("output/acemoglu_2001")
file <- "data/Acemoglu 2001_citation_cases.csv"
article <- "Acemoglu, Johnson & Robinson (2001)"
output <- "output/acemoglu_2001"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Audretsch 1996
dir.create("output/audretsch_1996")
file <- "data/Audretsch 1996_citation_cases.csv"
article <- "Audretsch and Feldman (1996)"
output <- "output/audretsch_1996"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Beck 1995
dir.create("output/beck_1995")
file <- "data/Beck 1995_citation_cases.csv"
article <- "Beck and Katz (1995)"
output <- "output/beck_1995"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Fearon 2003
dir.create("output/fearon_2003")
file <- "data/Fearon 2003_citation_cases.csv"
article <- "Fearon and Laitin (2003)"
output <- "output/fearon_2003"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Inglehart 2000
dir.create("output/inglehart_2000")
file <- "data/Inglehart 2000_citation_cases.csv"
article <- "Inglehart and Baker (2000)"
output <- "output/inglehart_2000"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
# Uzzi 1996
dir.create("output/uzzi_1996")
file <- "data/Uzzi 1996_citation_cases.csv"
article <- "Uzzi (1996)"
output <- "output/uzzi_1996"
analyze_citations(file, article, output)
#topic_analysis(file, article, output)
