{
    "contents" : "---\ntitle: \"Analysis of citations\"\nauthor: \"Pablo Barbera, Simon, Paul\"\ndate: \"January 15, 2016\"\noutput:\n  html_document: default\n  pdf_document:\n    keep_tex: yes\n  word_document: default\n---\n\nThis document illustrates different techniques of automated text analysis applied to the study of citation text.\n\nNote: some of the code here is based on [https://github.com/kbenoit/ITAUR](Ken Benoit's text analysis seminar).\n\nSetup:\n```{r}\n# Simon\n  # setwd(\"~/Dropbox/research/citations\")\n  # setwd(\"~/Dropbox/Uni/Forschung/2015 Citations\")\n\n# Pablo\n  # setwd(\"~/Dropbox/research/citations\")\n\n# Paul\n  setwd(\"C:/Users/paul/Desktop/citationdata\")\n\n\n# install.packages(\"quanteda\")\n# install.packages(\"stm\")\nrequire(quanteda, quietly=TRUE, warn.conflicts = FALSE)\nrequire(stm, quietly=TRUE)\nrequire(ggplot2, quietly=TRUE)\nrequire(scales)\nrequire(dplyr)\nrequire(stringr)\noptions(stringsAsFactors=F)\n```\n\nReading the dataset\n```{r}\ntf <- read.csv('citation_cases_Beck 1995.csv')\ntf$citation.case <- iconv(tf$citation.case, from='UTF-8', to='latin1', sub=\"\")\ntf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\\\1'))\ntf <- tf[!is.na(tf$year),] # deleting citations with empty years\ntf <- tf[!duplicated(tf$citation.case),] # deleting duplicates\n```\n\nExtracting author names (will be used as stopwords later) and year of each citation\n```{r}\nauthors <- tokenize(toLower(tf$document), removePunct=TRUE, removeNumbers=TRUE)\nauthors <- unique(unlist(authors))\ntf$year <- gsub('.*([0-9]{4}).*', tf$document, repl='\\\\1')\n```\n\nCleaning the citation text\n```{r}\n# tokenizing\ntokens <- tokenize(toLower(tf$citation.case), removePunct=TRUE, removeNumbers=TRUE)\n# removing stopwords, author names, and other frequent words\ntokens <- removeFeatures(tokens, \n\tc(stopwords(\"english\"), \"other\", \"others\", \"see\", \"also\", authors))\n# stemming?\n#tokens <- lapply(tokens, wordstem)\n# creating n-grams\nngrams <- lapply(tokens, ngrams, 1:3)\n# putting it all back together...\nngrams <- unlist(lapply(ngrams, paste, collapse=\" \"))\n# deleting empty citations...\ntodelete <- which(ngrams==\"\")\ntf <- tf[-todelete,]\ntokens <- tokens[-todelete]\nngrams <- ngrams[-todelete]\n```\n\nConstructing the document-feature matrix. (I add the title of the citing paper as the name of each document)\n```{r}\ncit <- corpus(ngrams)\ndocnames(cit) <- paste0(1:nrow(tf), '_', tf$document)\nsummary(cit)\ncitmat <- dfm(cit)\n```\n\nSome preliminary analysis... what are the most frequent features?\n```{r}\ntopfeatures(citmat, 30)\nplot(citmat, rot.per=0, scale=c(3.5, .3), max.words=50)\n```\n\n\nNow we move to run a topic model. The number of topics (K) is determined based on what value of K maximizes semantic coherence.\n\n```{r,  message=FALSE, warning=FALSE, results=\"hide\", cache=TRUE}\ncitstm <- convert(citmat, to=\"stm\")\nK <- 3:8\nout <- prepDocuments(citstm$documents, citstm$vocab, \n\ttf[,c(\"year\", \"document\", \"citation.case\")])\nmanymodels <- manyTopics(out$documents, out$vocab, K=K, verbose=FALSE,\n\tLDAbeta=FALSE, sigma.prior=1, seed=12345, runs=5)\n```\n```{r}\n# finding right number of topics\nchosen <- which.max(unlist(lapply(manymodels$semcoh, mean)))\ncat(K[chosen], 'number of topics is the best fit!')\nmodel <- manymodels$out[[chosen]]\n# topic proportions\napply(model$theta, 2, mean)\n```\n\nTo check the output of the model, we can look at the words more closely associated to each topic and the most representative documents for each topic\n```{r, tidy=TRUE}\n# output of model\nwords <- labelTopics(model, n=20)\n(apply(words$lift, 1, paste, collapse=\", \"))\n# most representative documents for each topic\nfindThoughts(model, out$meta$document)\n# now showing also the text of the documents\nfindThoughts(model, paste0(out$meta$document, ': ', out$meta$citation.case), n=3)\n```\n\nLooking at topic usage over time...\n```{r}\n# topics over time\ntopics <- data.frame(\n\tprop = c(model$theta),\n\tdocument = rep(1:nrow(out$meta), times=K[chosen]),\n\ttopic = rep(1:K[chosen], each=nrow(out$meta)),\n\tyear = rep(out$meta$year, times=K[chosen]))\nagg <- aggregate(topics$prop, \n\tby=list(topic=factor(topics$topic), year=topics$year),\n\tFUN=mean)\nnames(agg)[names(agg)==\"x\"] <- \"prop\"\n\nlibrary(ggplot2)\nlibrary(scales)\np <- ggplot(agg, aes(x=year, y=prop, group=topic, fill=topic))\npq <- p + geom_area(position=\"stack\") + theme_minimal() +\n\ttheme(axis.ticks.y=element_blank(), axis.title=element_blank(),\n\t\taxis.text.y=element_blank())\npq\n\np <- ggplot(agg, aes(x=year, y=prop, group=topic, color=topic))\npq <- p + geom_line() + theme_minimal() +\n\ttheme(axis.title.x=element_blank()) +\n\tscale_y_continuous(\"Average topic proportion\", labels=percent)\npq\n```\n\n\nFinally, we can do some basic sentiment analysis using the `qdap` package\n```{r}\nrequire(qdap, quietly = TRUE,  warn.conflicts = FALSE)\ntext <- unlist(lapply(tokens, paste, collapse=\" \"))\nsentiment <- polarity(text, grouping.var=tf$document)\nsentiment$group <- sentiment$group[order(sentiment$group$ave.polarity),]\nsentiment\n```\n\nOne way to validate the results is by checking the citations that are most negative and most positive\n```{r}\n# Most negative\ntf$citation.case[tf$document %in% head(sentiment$group$document, 3)]\n# Most positive\ntf$citation.case[tf$document %in% tail(sentiment$group$document, 3)]\n```\n\nWe can also look at average sentiment over time\n```{r}\nsentiment <- polarity(text, grouping.var=tf$year)\n\np <- ggplot(sentiment$group, aes(x=as.numeric(year), y=ave.polarity, group=1))\npq <- p + geom_point() + geom_path() + theme_minimal() +\n\ttheme(axis.title.x=element_blank()) + \n\tgeom_hline(yintercept=0, linetype=6) +\n\tscale_y_continuous(\"Average sentiment of citation\")\npq\n```\n\n\nNow, we study whether the citation is exclusive (just the paper of interest cited in the fragment) or one among several citations. We approximate the number of citations by looking after years (19XX or 20XX) in the fragment. In documents that use author-year citations, this should be a good indicator for references in the text\n```{r}\ntf$citation_counts <- str_count(tf$citation.case, \"\\\\b(19|20)[[:digit:]]{2}\\\\b\")\ntable(tf$citation_counts)\n```\n\nWe can validate whether this procedure worked by looking at some of the cases\n```{r}\ntf$citation.case[tf$citation_counts == 7]\ntf$citation.case[tf$citation_counts == 6]\ntf$citation.case[tf$citation_counts == 2][1:10]\n```\nThis looks good, although in case of very few hits (e.g., 2 hits), the original paper  take a more prominent place (which makes sense).\n\nWe can look whether the exclusiveness in citation status has changed over time\n```{r}\nby_years <- group_by(tf, year)\ntf_group <- summarize(by_years, mean_citations = mean(citation_counts, na.rm = TRUE))\np <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_citations))\np + geom_point() + geom_line() + theme_minimal() +\n\ttheme(axis.title.x=element_blank()) + \n\tscale_y_continuous(\"Average number of references in fragment\")\n```\n\nNext, we look for various signal words...\n```{r}\ntf$signal_example <- str_detect(tf$citation.case, \"([Ff]or example)|([Ff]or instance)|([Ee]\\\\.g\\\\.)\")\ntf$signal_positive <- str_detect(tf$citation.case, \"follow|recommend|validat|suggest|accordance|advice|demonstrate|confirm|support|in line with|based\")\n```\n\n\n... and see whether the prevalence of such citations evolves over time\n```{r}\nby_years <- group_by(tf, year)\ntf_group <- summarize(by_years, mean_signal_example = mean(signal_example, na.rm = TRUE), mean_signal_positive = mean(signal_positive, na.rm = TRUE))\np <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_signal_positive))\np + geom_point() + geom_line() + theme_minimal() +\n\ttheme(axis.title.x=element_blank()) + \n\tscale_y_continuous(\"Average number of citations with `positive' signal\")\n```\n\n\n\n",
    "created" : 1452879412082.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1197321115",
    "id" : "60D1A5D7",
    "lastKnownWriteTime" : 1452879881,
    "path" : "C:/Users/paul/Google Drive/Packages/citations/citation-analysis-beck.Rmd",
    "project_path" : "citation-analysis-beck.Rmd",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}