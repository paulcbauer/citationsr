{
    "contents" : "---\ntitle: \"Analysis of citations\"\nauthor: \"Pablo Barbera, Simon, Paul\"\ndate: \"January 15, 2016\"\noutput:\n  html_document: default\n  pdf_document:\n    keep_tex: yes\n  word_document: default\n---\n\n## Prerequesites\n\n* You need to install xpdf into the folder `C:\\Program Files\\xpdf`\n\n# Citations\nThe R package 'citations' comprises functions that can be used to analyze citation cases, i.e. citations within documents that cite a particular (or several) studies. These functions would normally be executed in a sequence.\n\n1. `extract_text()`: Extract text from citing documents (PDFs) and saves text in text files with the same name. \n1. `identify_citing_doc()`: Identifies citing documents (PDFs) either by finding a DOI and querying information using that DOI or by trying to extract title, journal, pages etc. from the PDF itself.\n2. `identify_study_doc()`: Runs through citing documents and identifes whether study of interest was really cited in there, i.e. whether it appears in the references. Produces a table with the extracted reference (if there has been) to the study.\n3. `delete_ref_section()` and `delete_running_heads()`: Clean up .txt files of citing documents (omitting/deleting reference section and running heads) to facilitate citation case extraction. \n5. `clean_text()`: Clean .txt files (e.g. replace abbreviation) to facilitate citation case extraction.\n8. `extract_citation_cases()`: Extract citations cases from citing documents (.txt files). \n9. Analyze the extracted citation cases: to be continued...\n\n* `clean_pdf_filenames` and `numeric_to_name` are unfinished functions. The former shall rename PDF files with problematic names. The latter shall convert citing documents from the numeric to the name citation system.\n\n\n# Extracting the citation cases\nThe data to run the functions across several studies/folders is stored in ```publicationdata.txt```. If you just want to run the stuff for the first folder/study replace ```1:6``` by ```1``` in the loops. Moreover, you can choose the number of docs for which you want to run the functions. The ```get_metadata_doc``` automatically only uses the text files that are present in the respective folders.\n\n```{r, eval=F, echo=T, message=F, cache=F}\nlibrary(citations)\nsetwd(\"C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data\")\npublicationdata <- read.table(\"publicationdata.txt\", sep=\";\",\n                                header = T, stringsAsFactors = F)\n\n\n\n#######################################\n## Extract text (several folders)\n\n    # Loop across rows of 'publicationdata'\n      for (i in 1:6){ \n      \n        # SPECIFY ARGUMENT FOR SINGLE FUNCTIONS\n          folder <- publicationdata[i,1]\n          study.title <- publicationdata[i,2]\n          authors <- publicationdata[i,3]\n          year <- publicationdata[i,4]  \n          \n          \n          \n          \n          # TEXT EXTRACTION\n            extract_text(folder = folder,  number = 40)\n          \n          \n  \n        \n          # GETTING METADATA\n            files <- list.files(folder, pattern = \"txt$\", full.names = T)\n            foo <- plyr::adply(files[1:length(files)], .margins = 1, .fun = get_metadata_doc, \n                             .progress = \"text\")\n      \n            # Replace NULL values in metadata tables\n            for(z in 1:nrow(foo)){for(y in 1:length(foo)){\n              if(is.null(foo[z,y][[1]])){foo[z,y][[1]] <- NA}\n            }}\n            # save metadatatables in data folder\n            save(foo, file = paste(\"./\", folder, \"_metadata.RData\", sep=\"\"))\n      \n\n\n\n          # RENAME DOCS USING METADATA\n            rename_docs(folder = folder)\n    \n\n\n        \n          # IDENTIFY STUDY IN DOCS\n            identify_study_doc(study.title = study.title,\n                             folder = folder) \n    \n\n          # CLEAN TEXT FILES BEFORE CASE EXTRACTION\n            delete_ref_section(folder = folder)\n            delete_running_heads(folder = folder)\n            clean_text(folder = folder)\n\n\n\n          # EXTRACT CITATION CASES\n            extract_citation_cases(folder = folder,\n                                     authorname = authors,\n                                     studyyear = year) \n    }\n```\n\n\n# Analysis of citation cases\n\nThis document illustrates different techniques of automated text analysis applied to the study of citation text.\n\nNote: some of the code here is based on [https://github.com/kbenoit/ITAUR](Ken Benoit's text analysis seminar).\n\n```{r, eval=F, echo=T, message=F, cache=F}\n# Simon\n  # setwd(\"~/Dropbox/research/citations\")\n  # setwd(\"~/Dropbox/Uni/Forschung/2015 Citations\")\n\n# Pablo\n  # setwd(\"~/Dropbox/research/citations\")\n\n# Paul\n  setwd(\"C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data\")\n\n\n# install.packages(\"quanteda\")\n# install.packages(\"stm\")\n# install.packages(\"qdap\")\n# install.packages(\"knitr\")\n# install.packages('digest')\nlibrary(quanteda, quietly=T, warn.conflicts = F)\nlibrary(stm, quietly=T, warn.conflicts = F)\nlibrary(ggplot2, quietly=T)\nlibrary(scales)\nlibrary(dplyr, warn.conflicts = F)\nlibrary(stringr)\nlibrary(knitr)\noptions(stringsAsFactors=F)\n```\n\n# Reading the dataset\n```{r, eval=F, echo=T, warning=F}\ntf <- read.csv('C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/data/Beck 1995_citation_cases.csv')\ntf$citation.case <- iconv(tf$citation.case, from='UTF-8', to='latin1', sub=\"\")\ntf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\\\1'))\ntf <- tf[!is.na(tf$year),] # deleting citations with empty years\ntf <- tf[!duplicated(tf$citation.case),] # deleting duplicates\n```\n\nThe table below illustrates what our data looks like. Document contains the corresponding citation and citation.case contains the citation cases that are grouped for the different citations.\n```{r, eval=F, echo=T}\nkable(tf[0:9,], format = 'markdown')\n```\n\n\n\n# Preliminary results: Structure of citation cases\n\nWe start with a simple count of citation cases per citing document. “Times cited” just counts the citing documents. In contrast the “citation case count” reflects the number of citation cases. One could argue that this number better reflects the impact of a study (or at least would add to a better measure thereof). In other words, when a study is mentioned once in a citing document it is less important than if it’s mentioned several times throughout the citing document. In our data set for (Beck and Katz 1995) there are `r #length(unique(tf$document))` citing documents and we identified a total of `r # length(tf$citation.case)` citation cases in them.\nThe figure below describes the distribution of the number of citation cases across documents. While most documents reference (Beck and Katz 1995) only once, there is a considerable number of documents with 2 or more citation cases going to an extreme of 19 citation cases. The document with the most citation cases is a methodological study focusing on the same topic (Tsay 2009) and it seems to make sense to argue that (Beck and Katz 1995) has a stronger impact on (Tsay 2009) than for the documents in which it was only cited once.\n\n\n```{r, eval=F, echo=T}\ntf$num <- 1 # there must be another way...\nx <- aggregate(tf[,\"num\"], by=list(tf$document), FUN=sum)\n# table(x$x)\nrange.x <- range(x$x)\nbreaks <- c(range.x[1]-1.5, range.x[1]:range.x[2]-0.5, range.x[2]+0.5)\nseq.x <- c(0, seq(range.x[1], range.x[2], 1))\nhist(x$x, xlab=\"N citation cases per document\", main = \"Citation cases: Beck & Katz 1995\", xaxt=\"n\", , breaks=breaks, cex.main=1, ylab=\"Frequency = N documents\") # ylim=c(0,700) \naxis(1,seq.x)\n```\n\nAnother interesting aspect is whether a citation case references exclusively to the study we are interested in or whether the study is co-referenced together with other studies. For instance, often articles contain citation cases in the introduction that point to a series of fundamental articles in the field, however, all of these appear together in a single citation case and subsequently these don’t appear in the paper again. The figure below illustrates that - as was to be expected - most citation cases only reference (Beck and Katz 1995). However, there also is a considerable number of citation cases in which they are co-referenced together with other studies. \n\n[comment]: <> (We approximate the number of citations by looking after years (19XX or 20XX) in the fragment. In documents that use author-year citations, this should be a good indicator for references in the text)\n\n\n```{r, eval=F, echo=T}\ntf$citation_counts <- str_count(tf$citation.case, \"(19|20)[0-9]{2}\") # there were 23 0-cases so I took out \\b but there are still 5..\n# table(tf$citation_counts)\n```\n\n[comment]: <> (We can validate whether this procedure worked by looking at some of the cases)\n```{r, eval=F, echo=T}\ntf$citation.case[tf$citation_counts == 7]\ntf$citation.case[tf$citation_counts == 6]\ntf$citation.case[tf$citation_counts == 2][1:10]\n```\n[comment]: <> (This looks good, although in case of very few hits (e.g., 2 hits), the original paper take a more prominent place (which makes sense). There are also hits with two studies of the same author, e.g. \"Beck and Katz (1995, 1996)\".)\n\n\n```{r, eval=F, echo=T}\nx <- tf$citation_counts\n# table(x)\nrange.x <- range(x)\nbreaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)\nseq.x <- c(seq(range.x[1], range.x[2], 1))\nhist(x, xlab=\"N references within citation case\", main = \"Citation cases: Beck & Katz 1995\", xaxt=\"n\", breaks = breaks,  cex.main=1, ylab=\"Frequency = N citation cases\") \naxis(1,seq.x)\n```\n\nWhile most citation cases, namely `r #table(tf$citation_counts)[2]` out of `r # sum(table(tf$citation_counts))`, refer to Beck and Katz (1995) exclusively, there are quite a few citation cases in which the authors reference several studies together with Beck and Katz.\nThe figure below plots the average number of references per citation cases across years. While there does not seem to be a systematic development it illustrates how we can various measures of qualitative impact over time.\n\n```{r, eval=F, echo=T}\nby_years <- group_by(tf, year)\ntf_group <- summarize(by_years, mean_citations = mean(citation_counts, na.rm = T))\np <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_citations))\np + geom_point() + geom_line() + theme_minimal() +\n\ttheme(axis.title.x=element_blank()) + \n\tscale_y_continuous(\"Average number of references in citation case\")\n```\n\nLength in terms of characters and words could be another interesting indicator. Following the rationale that authors will write more about studies that are more relevant to their article, one might argue that the length of citation cases is relevant. Below we plot the length of citation cases in terms of characters as well as words (very rough approximations) is plotted. Again we see that the variation is quite strong. While most citation cases contain between \n30 to 40 words, there are both citation cases that are much shorter and cases that are much longer. Additional analyses might identify patterns behind this variation in terms of content.\n\n```{r, eval=F, echo=T}\npar(mfrow=c(1,2))\nx <- nchar(tf$citation.case)\n# table(x)\nhist(x, xlab=\"Characters per citation case\", main = \"\\n\", breaks = 20,  cex.main=1, ylab=\"Frequency = N citation cases\") \n\nx <- sapply(str_extract_all(tf$citation.case, \"\\\\W+\"), length) # Very rough count of words\nhist(x, xlab=\"Words per citation case\", main = \"\\n\", breaks = 20,  cex.main=1, ylab=\"Frequency = N citation cases\") \n```\n\n\n# Preliminary results: Content of citation cases\nAbove we focused on the structural characteristics of citation cases (e.g. their number within citing documents, length etc.). More interesting and the main motivation for our project is their content. For instance, we would like to know why a particular study was cited. Moreover, we maybe interested in classifying the content, i.e. the citation cases: Positive vs. negative, right vs. wrong, relevant vs. irrelevant to name a few examples.\n\n```{r, eval=F, echo=T}\ntf$signal_example <- str_detect(tf$citation.case, \"([Ff]or example)|([Ff]or instance)|([Ee]\\\\.g\\\\.)\")\nsignal.words <- \"follow|recommend|validate|suggest|accordance|advice|demonstrate|confirm|support|in line with|based\"\ntf$signal_positive <- str_detect(tf$citation.case, signal.words)\n```\n\nBelow we ouline some preliminary ways of how we might go about investigating these questions. We try to identify certain words within citation cases that we consider informative. For instance, below we plot the share of citation cases across time in which a list of such words appear namely the words - `r #signal.words`. In other words, it displays how the share of citation cases the contain these words evolves across time. While this is just a preliminary analysis it illustrates that we can analyze the prevalence of certain terms within citation cases across time.\n\n\n```{r, eval=F, echo=T}\nby_years <- group_by(tf, year)\ntf_group <- summarize(by_years, mean_signal_example = mean(signal_example, na.rm = T), mean_signal_positive = mean(signal_positive, na.rm = T))\np <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_signal_positive))\np + geom_point() + geom_line() + theme_minimal() +\n\ttheme(axis.title.x=element_blank()) + \n\tscale_y_continuous(\"Average number of citations with `positive' signal\")\n```\n\n\nBut we may also turn to more \"sophisticated\" ways of analyzing textual data. For instance, we may try to get an overview of the most frequent words that appear within the citation cases as plotted in the figure below. Before conducting these more sophisticated analyses of our citation cases dataset, we apply standard text pre-processing techniques: tokenization after removing digits, converting to lowercase, and removing stopwords and author names. We then extract n-grams up to trigrams, which will be the features we consider in the analysis.\n\nOur study of the quality of citations relies on two different text analysis techniques: visualization of most common n-grams with wordclouds and  identification of general themes using topic modeling.\n\nA wordcloud displays the most common n-grams that appear in a given text (in this cases, citation cases), with the size of the words proportional to their frequency. As we show below, the wordcloud summarizes the reason why this paper is cited: researchers working with time-series cross-sectional data use the method described in the paper to estimate standard errors that are correct even in the presence of serial correlation within unit and heteroskedastic variance across units.\n\n[comment]: <> (Extracting author names (will be used as stopwords later) and year of each citation)\n```{r, eval=F, echo=T, message=F, warning=F, results=\"hide\"}\nauthors <- tokenize(toLower(tf$document), removePunct=T, removeNumbers=T)\nauthors <- unique(unlist(authors))\ntf$year <- gsub('.*([0-9]{4}).*', tf$document, repl='\\\\1')\n```\n[comment]: <> (Cleaning the citation text)\n```{r, eval=F, echo=T, message=F, warning=F, results=\"hide\"}\n# tokenizing\ntokens <- tokenize(toLower(tf$citation.case), removePunct=T, removeNumbers=T)\n# removing stopwords, author names, and other frequent words\ntokens <- removeFeatures(tokens, \n\tc(stopwords(\"english\"), \"other\", \"others\", \"see\", \"also\", authors))\n# stemming?\n#tokens <- lapply(tokens, wordstem)\n# creating n-grams\nngrams <- lapply(tokens, ngrams, 1:3)\n# putting it all back together...\nngrams <- unlist(lapply(ngrams, paste, collapse=\" \"))\n# deleting empty citations...\ntodelete <- which(ngrams==\"\")\ntf <- tf[-todelete,]\ntokens <- tokens[-todelete]\nngrams <- ngrams[-todelete]\n```\n[comment]: <> (Constructing the document-feature matrix. (I add the title of the citing paper as the name of each document))\n```{r, eval=F, echo=T, message=F, warning=F, results=\"hide\"}\ncit <- corpus(ngrams)\ndocnames(cit) <- paste0(1:nrow(tf), '_', tf$document)\n# summary(cit)\ncitmat <- dfm(cit)\n```\n```{r, eval=F, echo=T, comment=F, warning=F}\n# topfeatures(citmat, 10)\npar(mfrow=c(1,1))\nplot(citmat, rot.per=0, scale=c(3.5, .3), max.words=50)\n```\n\n\nSecond, we use topic modeling methods to summarize citation cases into broader categories or themes. This type of unsupervised models -- of which the most prominent example is Latent Dirichlet Allocation (Blei et al, 2003) -- identify clusters of n-grams that appear together across different documents. Each document is considered to be a probability distribution over different topics; and each topic is a probability distribution over n-grams. In this particular case, we use the Sparse Additive Generative (SAGE) model (Eisenstein et al, 2011), which has been shown to yield more semantically coherent topics when the documents are short, as it is the case here. The only parameter that needs to be fixed a priori with this type of models is the number of topics to be estimated. In our analysis, we estimate SAGE models with different numbers of topics, and then pick the model that yields a higher measure of semantic coherence (Mimno et al, 2011). We estimate all our models using the implementation in the stm package for R (Roberts et al, 2014). \n\nTables X and X below provide a summary of our results. First, we display the most common words associated with each topic. Note that our analysis suggests that 4 topics provide the best fit for this dataset. Then, we also identify the most representative documents for each topic. Our preliminary analysis suggest that the four main themes are: “time-series cross-section data” (topic 1), “why people use this method” (topic 3), “general discussion of time series methods” (topic 4), and “other” (topic 2).\n\n\n[comment]: <> (Now we move to run a topic model. The number of topics (K) is determined based on what value of K maximizes semantic coherence.)\n```{r,  eval=F, echo=T, message=F, warning=F, results=\"hide\"}\n# , cache=FRUE\ncitstm <- convert(citmat, to=\"stm\")\nK <- 3:6\nout <- prepDocuments(citstm$documents, citstm$vocab, \n\ttf[,c(\"year\", \"document\", \"citation.case\")])\nmanymodels <- manyTopics(out$documents, out$vocab, K=K, verbose=T,\n\tLDAbeta=F, sigma.prior=0.5, seed=777, runs=2, max.em.its=10, net.max.em.its=2) # , runs=5\n```\n```{r, eval=F, echo=T}\n# finding right number of topics\nchosen <- which.max(unlist(lapply(manymodels$semcoh, mean)))\n#cat(K[chosen], 'number of topics is the best fit!')\nmodel <- manymodels$out[[chosen]]\n# topic proportions\n#apply(model$theta, 2, mean)\n```\n\n```{r, eval=F, tidy=T, echo=T}\n# output of model\nwords <- labelTopics(model, n=20)\ntopictab <- data.frame(\n  \"Topic\" = 1:K[chosen],\n  \"Words\" = apply(words$lift, 1, paste, collapse=\", \"))\nkable(topictab, format = 'markdown')\n# most representative documents for each topic\n#findThoughts(model, out$meta$document)\n# now showing also the text of the documents\ndf <- findThoughts(model, paste0(out$meta$document, ': ', out$meta$citation.case), n=1)\ntopictab <- data.frame(\n  \"Topic\" = 1:K[chosen],\n  \"Representative document\" = unlist(df$docs))\nnames(topictab)[2] <- \"Representative document\"\nkable(topictab, format='markdown')\n\n```\n\n```{r, eval=F, echo=T}\n# topics over time\ntopics <- data.frame(\n\tprop = c(model$theta),\n\tdocument = rep(1:nrow(out$meta), times=K[chosen]),\n\ttopic = rep(1:K[chosen], each=nrow(out$meta)),\n\tyear = rep(out$meta$year, times=K[chosen]))\nagg <- aggregate(topics$prop, \n\tby=list(topic=factor(topics$topic), year=topics$year),\n\tFUN=mean)\nnames(agg)[names(agg)==\"x\"] <- \"prop\"\n\nlibrary(ggplot2)\nlibrary(scales)\np <- ggplot(agg, aes(x=year, y=prop, group=topic, fill=topic))\npq <- p + geom_area(position=\"stack\") + theme_minimal() +\n\ttheme(axis.ticks.y=element_blank(), axis.title=element_blank(),\n\t\taxis.text.y=element_blank())\npq\n\np <- ggplot(agg, aes(x=year, y=prop, group=topic, color=topic))\npq <- p + geom_line() + theme_minimal() +\n\ttheme(axis.title.x=element_blank()) +\n\tscale_y_continuous(\"Average topic proportion\", labels=percent)\npq\n```\n\n[comment]: <> (The final step in our analysis consists on applying sentiment analysis to our corpus of citation cases. This technique consists on counting the number of positive and negative words in each citation case, from a sentiment dictionary (Hu and Liu, 2004), and computing an index that measures their tone. Table 4 and Figure 2 summarize our results. This analysis illustrates an important limitation of this approach: it captures tone of the words in the citation case, but not necessarily whether the study is cited in a positive or negative light.)\n\n[comment]: <> (Finally, we can do some basic sentiment analysis using the `qdap` package)\n```{r, eval=F, echo=T, message=F, cache=T}\nrequire(qdap, quietly = T,  warn.conflicts = F)\ntext <- unlist(lapply(tokens, paste, collapse=\" \"))\nsentiment <- polarity(text, grouping.var=tf$document)\nsentiment$group <- sentiment$group[order(sentiment$group$ave.polarity),]\n#sentiment\n```\n\n[comment]: <> (One way to validate the results is by checking the citations that are most negative and most positive)\n```{r, eval=F, echo=T}\ndf <- data.frame(\n  Sentiment = rep(c(\"Positive\", \"Negative\"), each=3),\n  Citation = c(\n    tf$citation.case[tf$document %in% tail(sentiment$group$document, 3)],\n    tf$citation.case[tf$document %in% head(sentiment$group$document, 3)]))\nkable(df, \"markdown\")\n\n```\n\n[comment]: <> (We can also look at average sentiment over time)\n```{r, eval=F, echo=T}\nsentiment <- polarity(text, grouping.var=tf$year)\n\np <- ggplot(sentiment$group, aes(x=as.numeric(year), y=ave.polarity, group=1))\npq <- p + geom_point() + geom_path() + theme_minimal() +\n\ttheme(axis.title.x=element_blank()) + \n\tgeom_hline(yintercept=0, linetype=6) +\n\tscale_y_continuous(\"Average sentiment of citation\")\npq\n```\n\n\n\n\n",
    "created" : 1457638368180.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3306401801",
    "id" : "855CAAE2",
    "lastKnownWriteTime" : 1457667103,
    "path" : "C:/Users/paul/Google Drive/Research/2016_Quality_of_citations/analysis/ANALYSIS_paul.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}