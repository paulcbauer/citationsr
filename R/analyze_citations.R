#' Generates figures with summary of analysis of citation cases
#'
#' @param file 'citation_data.csv' that contains data on the citation cases.
#' @param article Name of cited article; e.g. Fearon (2003)
#' @param output folder where figures generated by function will be stored
#'
#'
#' @examples 
#' \dontrun{
#'  file <- "~/Google Drive/2016_Quality_of_citations/data/Fearon 2003_citation_cases.csv"
#'  article <- "Fearon and Laitin (2003)"
#'  output <- "fearon_2003"
#'  analyze_citations(file, article, output)
#' } 

analyze_citations <- function(file, article, output){

  require(ggplot2)
  require(scales)
  require(quanteda)

  # reading file and cleaning data
  tf <- read.csv(file, stringsAsFactors=F)
  # cleaning encoding and extracting year
  tf$citation.case <- iconv(tf$citation.case, from='UTF-8', to='latin1', sub="")
  tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
  message("Warning: ", sum(is.na(tf$year)), " citation cases with missing year will be excluded from analysis.")
  tf <- tf[!is.na(tf$year),] # deleting citations with empty years
  message("Warning: ", sum(duplicated(tf$citation.case)), " duplicated citation cases will be excluded from analysis.")
  tf <- tf[!duplicated(tf$citation.case),] # deleting duplicates
  message("Warning: ", sum(nchar(tf$citation.case)>1000), " citation cases longer than 1000 characters will be excluded from analysis.")
  tf <- tf[nchar(tf$citation.case)<=1000,] # deleting duplicates
  message("A total of ", nrow(tf), " citation cases will be included in the analysis.")

  # generating histogram with times cited within document
  x <- table(tf$document)
  range.x <- range(x)
  breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
  seq.x <- seq(range.x[1], range.x[2], 1)
  f1 <- paste0(output, '/01-times-cited-within-document.pdf')
  pdf(f1, height=4, width=6)
  par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.025)
  hist(x, xlab="N citation cases per document", 
    main = paste0("Citation cases: ", article), xaxt="n", 
    breaks=breaks, cex.main=1, ylab="Frequency = N documents") 
  axis(1,seq.x)
  dev.off()
  message("File generated: ", f1)

  # generating histograms for co-citations
  tf$citation_counts <- stringr::str_count(tf$citation.case, "(19|20)[0-9]{2}")
  x <- tf$citation_counts
  # table(x)
  range.x <- range(x)
  breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
  seq.x <- c(seq(range.x[1], range.x[2], 1))
  f2 <- paste0(output, '/02-co-citations-in-citation-case.pdf')
  pdf(f2, height=4, width=6)
  par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.025)
  hist(x, xlab="N references within citation case", 
    main = paste0("Citation cases: ", article), xaxt="n", breaks = breaks,  cex.main=1, ylab="Frequency = N citation cases") 
  axis(1,seq.x)
  dev.off()
  message("File generated: ", f2)

  # generating average number of references per citation over time
  tf_group <- aggregate(tf$citation_counts, by=list(year=tf$year), FUN=mean, na.rm=TRUE)
  p <- ggplot(tf_group, aes(x=as.numeric(year), y=x))
  pq <- p + geom_point() + geom_line() + theme_minimal() +
    theme(axis.title.x=element_blank()) + 
    scale_y_continuous("Average number of references in citation case")
  f3 <- paste0(output, '/03-co-citations-over-time.pdf')
  ggsave(pq, file=f3, height=4, width=6)
  message("File generated: ", f3)

  # figure with positive signals
  signal.words <- paste0("follow|recommend|validate|suggest|accordance|advice|demonstrate",
    "|confirm|support|in line with|based")
  tf$signal_positive <- grepl(signal.words, tf$citation.case)
  tf_group <- aggregate(tf$signal_positive, by=list(year=tf$year), FUN=mean, na.rm=TRUE)
  p <- ggplot(tf_group, aes(x=as.numeric(year), y=x))
  pq <- p + geom_point() + geom_line() + theme_minimal() +
    theme(axis.title.x=element_blank()) + 
    scale_y_continuous("Proportion of citations with `positive' signal",
      label=percent)
  f4 <- paste0(output, '/04-citations-with-positive-signal.pdf')
  ggsave(pq, file=f4, height=4, width=6)
  message("File generated: ", f4)

  # text cleaning
  authors <- tokenize(toLower(c(tf$document, article)), removePunct=T, removeNumbers=T)
  authors <- unique(unlist(authors))
  # tokenizing
  tokens <- tokenize(toLower(tf$citation.case), removePunct=T, removeNumbers=T)
  # removing stopwords, author names, and other frequent words
  tokens <- removeFeatures(tokens, 
    c(stopwords("english"), "other", "others", "see", "also", "u", authors))
  # stemming?
  #tokens <- lapply(tokens, wordstem)
  # creating n-grams
  ngrams <- lapply(tokens, ngrams, 1:3)
  # putting it all back together...
  ngrams <- unlist(lapply(ngrams, paste, collapse=" "))
  # constructing the DFM
  cit <- corpus(ngrams)
  docnames(cit) <- paste0(1:nrow(tf), '_', tf$document)
  # summary(cit)
  citmat <- dfm(cit)

  # word cloud
  f5 <- paste0(output, '/05-citations-word-cloud.pdf')
  pdf(f5, height=4, width=4)
  par (mar=c(0,0,0,0))
  plot(citmat, rot.per=0, scale=c(3, .3), max.words=80)
  dev.off()
  message("File generated: ", f5)

  # sentiment analysis
  dict <- qdapDictionaries::key.pol
  mydict <- dictionary(list(negative = dict$x[dict$y==-1],
                          postive = dict$x[dict$y==1]))
  myDfm <- dfm(cit, dictionary = mydict)
  tf$neg <- as.numeric(myDfm[,1])
  tf$pos <- as.numeric(myDfm[,2])
  tf$score <- (tf$pos - tf$neg)

  tf_group <- aggregate(tf$score, by=list(year=tf$year), FUN=mean, na.rm=TRUE)
  p <- ggplot(tf_group, aes(x=as.numeric(year), y=x))
  pq <- p + geom_point() + geom_line() + theme_minimal() +
    theme(axis.title.x=element_blank()) + 
    scale_y_continuous("Average sentiment in citations")
  f6 <- paste0(output, '/06-co-citations-over-time.pdf')
  ggsave(pq, file=f6, height=4, width=6)
  message("File generated: ", f6) 


}

