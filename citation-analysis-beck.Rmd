---
title: "Analysis of citations"
author: "Pablo Barbera, Simon, Paul"
date: "January 15, 2016"
output:
  html_document: default
  pdf_document:
    keep_tex: yes
  word_document: default
---

This document illustrates different techniques of automated text analysis applied to the study of citation text.

Note: some of the code here is based on [https://github.com/kbenoit/ITAUR](Ken Benoit's text analysis seminar).

```{r, eval=TRUE, echo=FALSE, message=FALSE, cache=T}
# Simon
  # setwd("~/Dropbox/research/citations")
  # setwd("~/Dropbox/Uni/Forschung/2015 Citations")

# Pablo
  # setwd("~/Dropbox/research/citations")

# Paul
  setwd("C:/Users/pbauer/Google Drive/Research/2016_Quality_of_citations/data")


# install.packages("quanteda")
# install.packages("stm")
# install.packages("qdap")
# install.packages("knitr")
# install.packages('digest')
library(quanteda, quietly=TRUE, warn.conflicts = FALSE)
library(stm, quietly=TRUE, warn.conflicts = FALSE)
library(ggplot2, quietly=TRUE)
library(scales)
library(dplyr, warn.conflicts = FALSE)
library(stringr)
library(knitr)
options(stringsAsFactors=F)
```

# Reading the dataset
```{r, eval=T, echo=F, warning=F}
tf <- read.csv('C:/Users/pbauer/Google Drive/Research/2016_Quality_of_citations/data/Beck 1995_citation_cases.csv')
tf$citation.case <- iconv(tf$citation.case, from='UTF-8', to='latin1', sub="")
tf$year <- as.numeric(gsub('.*([0-9]{4}).*', tf$document, repl='\\1'))
tf <- tf[!is.na(tf$year),] # deleting citations with empty years
tf <- tf[!duplicated(tf$citation.case),] # deleting duplicates
```

The table below illustrates what our data looks like. Document contains the corresponding citation and citation.case contains the citation cases that are grouped for the different citations.
```{r, eval=T, echo=F}
kable(tf[0:9,], format = 'markdown')
```



# Preliminary results: Structure of citation cases

We start with a simple count of citation cases per citing document. “Times cited” just counts the citing documents. In contrast the “citation case count” reflects the number of citation cases. One could argue that this number better reflects the impact of a study (or at least would add to a better measure thereof). In other words, when a study is mentioned once in a citing document it is less important than if it’s mentioned several times throughout the citing document. In our data set for (Beck and Katz 1995) there are `r length(unique(tf$document))` citing documents and we identified a total of `r length(tf$citation.case)` citation cases in them.
The figure below describes the distribution of the number of citation cases across documents. While most documents reference (Beck and Katz 1995) only once, there is a considerable number of documents with 2 or more citation cases going to an extreme of 19 citation cases. The document with the most citation cases is a methodological study focusing on the same topic (Tsay 2009) and it seems to make sense to argue that (Beck and Katz 1995) has a stronger impact on (Tsay 2009) than for the documents in which it was only cited once.


```{r, eval=T, echo=F}
tf$num <- 1 # there must be another way...
x <- aggregate(tf[,"num"], by=list(tf$document), FUN=sum)
# table(x$x)
range.x <- range(x$x)
breaks <- c(range.x[1]-1.5, range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(0, seq(range.x[1], range.x[2], 1))
hist(x$x, xlab="N citation cases per document", main = "Citation cases: Beck & Katz 1995", xaxt="n", , breaks=breaks, cex.main=1, ylab="Frequency = N documents") # ylim=c(0,700) 
axis(1,seq.x)
```

Another interesting aspect is whether a citation case references exclusively to the study we are interested in or whether the study is co-referenced together with other studies. For instance, often articles contain citation cases in the introduction that point to a series of fundamental articles in the field, however, all of these appear together in a single citation case and subsequently these don’t appear in the paper again. The figure below illustrates that - as was to be expected - most citation cases only reference (Beck and Katz 1995). However, there also is a considerable number of citation cases in which they are co-referenced together with other studies. 

[comment]: <> (We approximate the number of citations by looking after years (19XX or 20XX) in the fragment. In documents that use author-year citations, this should be a good indicator for references in the text)


```{r, eval=T, echo=F}
tf$citation_counts <- str_count(tf$citation.case, "(19|20)[0-9]{2}") # there were 23 0-cases so I took out \b but there are still 5..
# table(tf$citation_counts)
```

[comment]: <> (We can validate whether this procedure worked by looking at some of the cases)
```{r, eval=F, echo=F}
tf$citation.case[tf$citation_counts == 7]
tf$citation.case[tf$citation_counts == 6]
tf$citation.case[tf$citation_counts == 2][1:10]
```
[comment]: <> (This looks good, although in case of very few hits (e.g., 2 hits), the original paper take a more prominent place (which makes sense). There are also hits with two studies of the same author, e.g. "Beck and Katz (1995, 1996)".)


```{r, eval=T, echo=F}
x <- tf$citation_counts
# table(x)
range.x <- range(x)
breaks <- c(range.x[1]:range.x[2]-0.5, range.x[2]+0.5)
seq.x <- c(seq(range.x[1], range.x[2], 1))
hist(x, xlab="N references within citation case", main = "Citation cases: Beck & Katz 1995", xaxt="n", breaks = breaks,  cex.main=1, ylab="Frequency = N citation cases") 
axis(1,seq.x)
```

While most citation cases, namely `r table(tf$citation_counts)[2]` out of `r sum(table(tf$citation_counts))`, refer to Beck and Katz (1995) exclusively, there are quite a few citation cases in which the authors reference several studies together with Beck and Katz.
The figure below plots the average number of references per citation cases across years. While there does not seem to be a systematic development it illustrates how we can various measures of qualitative impact over time.

```{r, eval=T, echo=F}
by_years <- group_by(tf, year)
tf_group <- summarize(by_years, mean_citations = mean(citation_counts, na.rm = TRUE))
p <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_citations))
p + geom_point() + geom_line() + theme_minimal() +
	theme(axis.title.x=element_blank()) + 
	scale_y_continuous("Average number of references in citation case")
```

Length in terms of characters and words could be another interesting indicator. Following the rationale that authors will write more about studies that are more relevant to their article, one might argue that the length of citation cases is relevant. Below we plot the length of citation cases in terms of characters as well as words (very rough approximations) is plotted. Again we see that the variation is quite strong. While most citation cases contain between 
30 to 40 words, there are both citation cases that are much shorter and cases that are much longer. Additional analyses might identify patterns behind this variation in terms of content.

```{r, eval=T, echo=F}
par(mfrow=c(1,2))
x <- nchar(tf$citation.case)
# table(x)
hist(x, xlab="Characters per citation case", main = "\n", breaks = 20,  cex.main=1, ylab="Frequency = N citation cases") 

x <- sapply(str_extract_all(tf$citation.case, "\\W+"), length) # Very rough count of words
hist(x, xlab="Words per citation case", main = "\n", breaks = 20,  cex.main=1, ylab="Frequency = N citation cases") 
```


# Preliminary results: Content of citation cases
Above we focused on the structural characteristics of citation cases (e.g. their number within citing documents, length etc.). More interesting and the main motivation for our project is their content. For instance, we would like to know why a particular study was cited. Moreover, we maybe interested in classifying the content, i.e. the citation cases: Positive vs. negative, right vs. wrong, relevant vs. irrelevant to name a few examples.

```{r, eval=T, echo=F}
tf$signal_example <- str_detect(tf$citation.case, "([Ff]or example)|([Ff]or instance)|([Ee]\\.g\\.)")
signal.words <- "follow|recommend|validate|suggest|accordance|advice|demonstrate|confirm|support|in line with|based"
tf$signal_positive <- str_detect(tf$citation.case, signal.words)
```

Below we ouline some preliminary ways of how we might go about investigating these questions. We try to identify certain words within citation cases that we consider informative. For instance, below we plot the share of citation cases across time in which a list of such words appear namely the words - `r signal.words`. In other words, it displays how the share of citation cases the contain these words evolves across time. While this is just a preliminary analysis it illustrates that we can analyze the prevalence of certain terms within citation cases across time.


```{r, eval=T, echo=F}
by_years <- group_by(tf, year)
tf_group <- summarize(by_years, mean_signal_example = mean(signal_example, na.rm = TRUE), mean_signal_positive = mean(signal_positive, na.rm = TRUE))
p <- ggplot(tf_group, aes(x=as.numeric(year), y=mean_signal_positive))
p + geom_point() + geom_line() + theme_minimal() +
	theme(axis.title.x=element_blank()) + 
	scale_y_continuous("Average number of citations with `positive' signal")
```


But we may also turn to more "sophisticated" ways of analyzing textual data. For instance, we may try to get an overview of the most frequent words that appear within the citation cases as plotted in the figure below. Before conducting these more sophisticated analyses of our citation cases dataset, we apply standard text pre-processing techniques: tokenization after removing digits, converting to lowercase, and removing stopwords and author names. We then extract n-grams up to trigrams, which will be the features we consider in the analysis.

Our study of the quality of citations relies on two different text analysis techniques: visualization of most common n-grams with wordclouds and  identification of general themes using topic modeling.

A wordcloud displays the most common n-grams that appear in a given text (in this cases, citation cases), with the size of the words proportional to their frequency. As we show below, the wordcloud summarizes the reason why this paper is cited: researchers working with time-series cross-sectional data use the method described in the paper to estimate standard errors that are correct even in the presence of serial correlation within unit and heteroskedastic variance across units.

[comment]: <> (Extracting author names (will be used as stopwords later) and year of each citation)
```{r, eval=T, echo=F, message=FALSE, warning=FALSE, results="hide"}
authors <- tokenize(toLower(tf$document), removePunct=TRUE, removeNumbers=TRUE)
authors <- unique(unlist(authors))
tf$year <- gsub('.*([0-9]{4}).*', tf$document, repl='\\1')
```
[comment]: <> (Cleaning the citation text)
```{r, eval=T, echo=F, message=FALSE, warning=FALSE, results="hide"}
# tokenizing
tokens <- tokenize(toLower(tf$citation.case), removePunct=TRUE, removeNumbers=TRUE)
# removing stopwords, author names, and other frequent words
tokens <- removeFeatures(tokens, 
	c(stopwords("english"), "other", "others", "see", "also", authors))
# stemming?
#tokens <- lapply(tokens, wordstem)
# creating n-grams
ngrams <- lapply(tokens, ngrams, 1:3)
# putting it all back together...
ngrams <- unlist(lapply(ngrams, paste, collapse=" "))
# deleting empty citations...
todelete <- which(ngrams=="")
tf <- tf[-todelete,]
tokens <- tokens[-todelete]
ngrams <- ngrams[-todelete]
```
[comment]: <> (Constructing the document-feature matrix. (I add the title of the citing paper as the name of each document))
```{r, eval=T, echo=F, message=FALSE, warning=FALSE, results="hide"}
cit <- corpus(ngrams)
docnames(cit) <- paste0(1:nrow(tf), '_', tf$document)
# summary(cit)
citmat <- dfm(cit)
```
```{r, eval=T, echo=F, comment=FALSE, warning=FALSE}
# topfeatures(citmat, 10)
par(mfrow=c(1,1))
plot(citmat, rot.per=0, scale=c(3.5, .3), max.words=50)
```


Second, we use topic modeling methods to summarize citation cases into broader categories or themes. This type of unsupervised models -- of which the most prominent example is Latent Dirichlet Allocation (Blei et al, 2003) -- identify clusters of n-grams that appear together across different documents. Each document is considered to be a probability distribution over different topics; and each topic is a probability distribution over n-grams. In this particular case, we use the Sparse Additive Generative (SAGE) model (Eisenstein et al, 2011), which has been shown to yield more semantically coherent topics when the documents are short, as it is the case here. The only parameter that needs to be fixed a priori with this type of models is the number of topics to be estimated. In our analysis, we estimate SAGE models with different numbers of topics, and then pick the model that yields a higher measure of semantic coherence (Mimno et al, 2011). We estimate all our models using the implementation in the stm package for R (Roberts et al, 2014). 

Tables X and X below provide a summary of our results. First, we display the most common words associated with each topic. Note that our analysis suggests that 4 topics provide the best fit for this dataset. Then, we also identify the most representative documents for each topic. Our preliminary analysis suggest that the four main themes are: “time-series cross-section data” (topic 1), “why people use this method” (topic 3), “general discussion of time series methods” (topic 4), and “other” (topic 2).


[comment]: <> (Now we move to run a topic model. The number of topics (K) is determined based on what value of K maximizes semantic coherence.)
```{r,  eval=T, echo=FALSE, message=FALSE, warning=FALSE, results="hide"}
# , cache=TRUE
citstm <- convert(citmat, to="stm")
K <- 3:6
out <- prepDocuments(citstm$documents, citstm$vocab, 
	tf[,c("year", "document", "citation.case")])
manymodels <- manyTopics(out$documents, out$vocab, K=K, verbose=TRUE,
	LDAbeta=FALSE, sigma.prior=0.5, seed=777, runs=2, max.em.its=10, net.max.em.its=2) # , runs=5
```
```{r, echo=FALSE}
# finding right number of topics
chosen <- which.max(unlist(lapply(manymodels$semcoh, mean)))
#cat(K[chosen], 'number of topics is the best fit!')
model <- manymodels$out[[chosen]]
# topic proportions
#apply(model$theta, 2, mean)
```

```{r, eval=T, tidy=TRUE, echo=FALSE}
# output of model
words <- labelTopics(model, n=20)
topictab <- data.frame(
  "Topic" = 1:K[chosen],
  "Words" = apply(words$lift, 1, paste, collapse=", "))
kable(topictab, format = 'markdown')
# most representative documents for each topic
#findThoughts(model, out$meta$document)
# now showing also the text of the documents
df <- findThoughts(model, paste0(out$meta$document, ': ', out$meta$citation.case), n=1)
topictab <- data.frame(
  "Topic" = 1:K[chosen],
  "Representative document" = unlist(df$docs))
names(topictab)[2] <- "Representative document"
kable(topictab, format='markdown')

```

```{r, eval=F, echo=F}
# topics over time
topics <- data.frame(
	prop = c(model$theta),
	document = rep(1:nrow(out$meta), times=K[chosen]),
	topic = rep(1:K[chosen], each=nrow(out$meta)),
	year = rep(out$meta$year, times=K[chosen]))
agg <- aggregate(topics$prop, 
	by=list(topic=factor(topics$topic), year=topics$year),
	FUN=mean)
names(agg)[names(agg)=="x"] <- "prop"

library(ggplot2)
library(scales)
p <- ggplot(agg, aes(x=year, y=prop, group=topic, fill=topic))
pq <- p + geom_area(position="stack") + theme_minimal() +
	theme(axis.ticks.y=element_blank(), axis.title=element_blank(),
		axis.text.y=element_blank())
pq

p <- ggplot(agg, aes(x=year, y=prop, group=topic, color=topic))
pq <- p + geom_line() + theme_minimal() +
	theme(axis.title.x=element_blank()) +
	scale_y_continuous("Average topic proportion", labels=percent)
pq
```

[comment]: <> (The final step in our analysis consists on applying sentiment analysis to our corpus of citation cases. This technique consists on counting the number of positive and negative words in each citation case, from a sentiment dictionary (Hu and Liu, 2004), and computing an index that measures their tone. Table 4 and Figure 2 summarize our results. This analysis illustrates an important limitation of this approach: it captures tone of the words in the citation case, but not necessarily whether the study is cited in a positive or negative light.)

[comment]: <> (Finally, we can do some basic sentiment analysis using the `qdap` package)
```{r, eval=F, echo=F, message=FALSE, cache=TRUE}
require(qdap, quietly = TRUE,  warn.conflicts = FALSE)
text <- unlist(lapply(tokens, paste, collapse=" "))
sentiment <- polarity(text, grouping.var=tf$document)
sentiment$group <- sentiment$group[order(sentiment$group$ave.polarity),]
#sentiment
```

[comment]: <> (One way to validate the results is by checking the citations that are most negative and most positive)
```{r, eval=F, echo=FALSE}
df <- data.frame(
  Sentiment = rep(c("Positive", "Negative"), each=3),
  Citation = c(
    tf$citation.case[tf$document %in% tail(sentiment$group$document, 3)],
    tf$citation.case[tf$document %in% head(sentiment$group$document, 3)]))
kable(df, "markdown")

```

[comment]: <> (We can also look at average sentiment over time)
```{r, eval=F, echo=FALSE}
sentiment <- polarity(text, grouping.var=tf$year)

p <- ggplot(sentiment$group, aes(x=as.numeric(year), y=ave.polarity, group=1))
pq <- p + geom_point() + geom_path() + theme_minimal() +
	theme(axis.title.x=element_blank()) + 
	geom_hline(yintercept=0, linetype=6) +
	scale_y_continuous("Average sentiment of citation")
pq
```




